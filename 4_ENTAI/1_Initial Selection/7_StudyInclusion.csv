Publication Year,Title,Publication Title,Abstract Note
2025,"Comparison of ChatGPT-4, Copilot, Bard and Gemini Ultra on an Otolaryngology Question Bank.",Clinical otolaryngology : official journal of ENT-UK ; official journal of Netherlands Society for Oto-Rhino-Laryngology & Cervico-Facial Surgery,"OBJECTIVE: To compare the performance of Google Bard, Microsoft Copilot, GPT-4 with vision (GPT-4) and Gemini Ultra on the OTO Chautauqua, a student-created,  faculty-reviewed otolaryngology question bank. STUDY DESIGN: Comparative  performance evaluation of different LLMs. SETTING: N/A. PARTICIPANTS: N/A.  METHODS: Large language models (LLMs) are being extensively tested in medical  education. However, their accuracy and effectiveness remain understudied,  particularly in otolaryngology. This study involved inputting 350  single-best-answer multiple choice questions, including 18 image-based questions,  into four LLMS. Questions were sourced from six independent question banks  related to (a) rhinology, (b) head and neck oncology, (c) endocrinology, (d)  general otolaryngology, (e) paediatrics, (f) otology, (g) facial plastics,  reconstruction and (h) trauma. LLMs were instructed to provide an output  reasoning for their answers, the length of which was recorded. RESULTS: Aggregate  and subgroup analysis revealed that Gemini (79.8%) outperformed the other LLMs,  followed by GPT-4 (71.1%), Copilot (68.0%), and Bard (65.1%) in accuracy. The  LLMs had significantly different average response lengths, with Bard  (x̄ = 1685.24) being the longest and no difference between GPT-4 (x̄ = 827.34)  and Copilot (x̄ = 904.12). Gemini's longer responses (x̄ =1291.68) included  explanatory images and links. Gemini and GPT-4 correctly answered image-based  questions (n = 18), unlike Copilot and Bard, highlighting their adaptability and  multimodal capabilities. CONCLUSION: Gemini outperformed the other LLMs in terms  of accuracy, followed by GPT-4, Copilot and Bard. GPT-4, although it has the  second-highest accuracy, provides concise and relevant explanations. Despite the  promising performance of LLMs, medical learners should cautiously assess accuracy  and decision-making reliability."
2024,Voice EHR: introducing multimodal audio data for health.,Frontiers in digital health,"INTRODUCTION: Artificial intelligence (AI) models trained on audio data may have the potential to rapidly perform clinical tasks, enhancing medical  decision-making and potentially improving outcomes through early detection.  Existing technologies depend on limited datasets collected with expensive  recording equipment in high-income countries, which challenges deployment in  resource-constrained, high-volume settings where audio data may have a profound  impact on health equity. METHODS: This report introduces a novel protocol for  audio data collection and a corresponding application that captures health  information through guided questions. RESULTS: To demonstrate the potential of  Voice EHR as a biomarker of health, initial experiments on data quality and  multiple case studies are presented in this report. Large language models (LLMs)  were used to compare transcribed Voice EHR data with data (from the same  patients) collected through conventional techniques like multiple choice  questions. Information contained in the Voice EHR samples was consistently rated  as equally or more relevant to a health evaluation. DISCUSSION: The HEAR  application facilitates the collection of an audio electronic health record  (""Voice EHR"") that may contain complex biomarkers of health from conventional  voice/respiratory features, speech patterns, and spoken language with semantic  meaning and longitudinal context-potentially compensating for the typical  limitations of unimodal clinical datasets."
2025,Assessing adult sinusitis guidelines: A comparative analysis of AAO-HNS and AI Chatbots.,American journal of otolaryngology,"OBJECTIVE: To compare the guidelines offered by the American Academy of Otolaryngology-Head and Neck Surgery Foundation (AAO-HNS) on adult sinusitis to  chatbots. METHODS: ChatGPT-3.5, ChatGPT-4.0, Bard, and Llama 2 represent openly  accessible large language model-based chatbots. Accuracy, over-conclusiveness,  supplemental, and incompleteness of chatbot responses were compared to the  AAO-HNS Adult sinusitis clinical guidelines. RESULTS: 12 guidelines consisting of  30 questions from the AAO-HNS were compared to 4 different chatbots. Adherence to  AAO-HNS guidelines varied, with Llama 2 providing 80 % accurate responses, BARD  83.3 %, ChatGPT-4.0 80 %, and ChatGPT-3.5 73.3 %. Over-conclusive responses were  minimal, with only one instance each from Llama 2 and ChatGPT-4.0. However, rates  of incomplete responses varied, with Llama 2 exhibiting the highest at 40 %,  followed by ChatGPT-4.0 at 33.3 %, BARD at 23.3 %, and ChatGPT-3.5 at 36.7 %.  Fisher's Exact Test analysis revealed significant deviations from the guideline  standard, with less accuracy (p = 0.012 for Llama 2, p = 0.026 for BARD,  p = 0.012 for ChatGPT-4.0, p = 0.002 for ChatGPT-3.5), inclusion of supplemental  data (p < 0.001 for all), and less completeness (p < 0.01 for all) across all  chatbots, indicating potential areas for enhancement in their performance.  CONCLUSION: Although AI chatbots like Llama 2, Bard, and ChatGPT exhibit  potential in sharing health-related information, their present performance in  responding to clinical concerns concerning adult rhinosinusitis is not up to par  with recognized clinical criteria. Future revisions should focus on addressing  these shortcomings and placing an emphasis on accuracy, completeness, and  conformity with evidence-based practices."
2025,"Assessment of decision-making with locally run and web-based large language models versus human board recommendations in otorhinolaryngology, head and neck  surgery.",European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"INTRODUCTION: Tumor boards are a cornerstone of modern cancer treatment. Given their advanced capabilities, the role of Large Language Models (LLMs) in  generating tumor board decisions for otorhinolaryngology (ORL) head and neck  surgery is gaining increasing attention. However, concerns over data protection  and the use of confidential patient information in web-based LLMs have restricted  their widespread adoption and hindered the exploration of their full potential.  In this first study of its kind we compared standard human multidisciplinary  tumor board recommendations (MDT) against a web-based LLM (ChatGPT-4o) and a  locally run LLM (Llama 3) addressing data protection concerns. MATERIAL AND  METHODS: Twenty-five simulated tumor board cases were presented to an MDT  composed of specialists from otorhinolaryngology, craniomaxillofacial surgery,  medical oncology, radiology, radiation oncology, and pathology. This  multidisciplinary team provided a comprehensive analysis of the cases. The same  cases were input into ChatGPT-4o and Llama 3 using structured prompts, and the  concordance between the LLMs' and MDT's recommendations was assessed. Four MDT  members evaluated the LLMs' recommendations in terms of medical adequacy (using a  six-point Likert scale) and whether the information provided could have  influenced the MDT's original recommendations. RESULTS: ChatGPT-4o showed 84%  concordance (21 out of 25 cases) and Llama 3 demonstrated 92% concordance (23 out  of 25 cases) with the MDT in distinguishing between curative and palliative  treatment strategies. In 64% of cases (16/25) ChatGPT-4o and in 60% of cases  (15/25) Llama, identified all first-line therapy options considered by the MDT,  though with varying priority. ChatGPT-4o presented all the MDT's first-line  therapies in 52% of cases (13/25), while Llama 3 offered a homologous treatment  strategy in 48% of cases (12/25). Additionally, both models proposed at least one  of the MDT's first-line therapies as their top recommendation in 28% of cases  (7/25). The ratings for medical adequacy yielded a mean score of 4.7 (IQR: 4-6)  for ChatGPT-4o and 4.3 (IQR: 3-5) for Llama 3. In 17% of the assessments  (33/200), MDT members indicated that the LLM recommendations could potentially  enhance the MDT's decisions. DISCUSSION: This study demonstrates the capability  of both LLMs to provide viable therapeutic recommendations in ORL head and neck  surgery. Llama 3, operating locally, bypasses many data protection issues and  shows promise as a clinical tool to support MDT decisions. However at present,  LLMs should augment rather than replace human decision-making."
2025,Automated Evaluation of Antibiotic Prescribing Guideline Concordance in Pediatric Sinusitis Clinical Notes.,Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing,"BACKGROUND: Ensuring antibiotics are prescribed only when necessary is crucial for maintaining their effectiveness and is a key focus of public health  initiatives worldwide. In cases of sinusitis, among the most common reasons for  antibiotic prescriptions in children, healthcare providers must distinguish  between bacterial and viral causes based on clinical signs and symptoms. However,  due to the overlap between symptoms of acute sinusitis and viral upper  respiratory infections, antibiotics are often over-prescribed. OBJECTIVES:  Currently, there are no electronic health record (EHR)-based methods, such as lab  tests or ICD-10 codes, to retroactively assess the appropriateness of  prescriptions for sinusitis, making manual chart reviews the only available  method for evaluation, which is time-intensive and not feasible at a large scale.  In this study, we propose using natural language processing to automate this  assessment. METHODS: We developed, trained, and evaluated generative models to  classify the appropriateness of antibiotic prescriptions in 300 clinical notes  from pediatric patients with sinusitis seen at a primary care practice in the  Children's Hospital of Philadelphia network. We utilized standard prompt  engineering techniques, including few-shot learning and chain-of-thought  prompting, to refine an initial prompt. Additionally, we employed  Parameter-Efficient Fine-Tuning to train a medium-sized generative model Llama 3  70B-instruct. RESULTS: While parameter-efficient fine-tuning did not enhance  performance, the combination of few-shot learning and chain-of-thought prompting  proved beneficial. Our best results were achieved using the largest generative  model publicly available to date, the Llama 3.1 405B-instruct. On our evaluation  set, the model correctly identified 94.7% of the 152 notes where antibiotic  prescription was appropriate and 66.2% of the 83 notes where it was not  appropriate. However, 15 notes that were insufficiently, vaguely, or ambiguously  documented by physicians posed a challenge to our model, as none were accurately  classified. CONCLUSION: Our generative model demonstrated good performance in the  challenging task of chart review. This level of performance may be sufficient for  deploying the model within the EHR, where it can assist physicians in real-time  to prescribe antibiotics in concordance with the guidelines, or for monitoring  antibiotic stewardship on a large scale."
2024,Enhancing Multilingual Patient Education: ChatGPT's Accuracy and Readability for SSNHL Queries in English and Spanish.,OTO open,"OBJECTIVE: This study investigates ChatGPT's accuracy, readability, understandability, and actionability in responding to patient queries on sudden  sensorineural hearing loss (SSNHL) in English and Spanish, when compared to  Google responses. The objective is to address concerns regarding its proficiency  in addressing medical inquiries when presented in a language divergent from its  primary programming. STUDY DESIGN: Observational. SETTING: Virtual environment.  METHODS: Using ChatGPT 3.5 and Google, questions from the AAO-HNSF guidelines  were presented in English and Spanish. Responses were graded by 2  otolaryngologists proficient in both languages using a 4-point Likert scale and  the PEMAT-P tool. To ensure uniform application of the Likert scale, a third  independent evaluator reviewed the consistency in grading. Readability was  evaluated using 3 different tools specific to each language. IBM SPSS Version 29  was used for statistical analysis using one-way analysis of variance. RESULTS:  Across both languages, the responses displayed a native-level language  proficiency. Accuracy was comparable between sources and languages. Google's  Spanish responses had better readability (effect size 0.35, P < .001), while  Google's English responses were more understandable (effect size 0.67, P = .018).  ChatGPT's English responses demonstrated the highest level of actionability  (60%), though not significantly different when compared to other sources (effect  size 0.47, P = .14). CONCLUSION: ChatGPT offers patients comprehensive and  guideline-conforming answers to SSNHL patient medical queries in the 2 most  spoken languages in the United States. However, improvements in its readability  and understandability are warranted for more accessible patient education."
2024,Utility of an Artificial Intelligence Language Model for Post-Operative Patient Instructions Following Facial Trauma.,Craniomaxillofacial trauma & reconstruction,"STUDY DESIGN: Qualitative Descriptive Study. OBJECTIVE: To evaluate the utility of post-operative instructions (POIs) for facial trauma provided by the language  model ChatGPT as compared to those from electronic medical record (EMR) templates  and Google search engine. METHODS: POIs for four common facial trauma procedures  (mandibular fracture, maxillary fracture, nasal fracture, and facial laceration)  were generated by ChatGPT, extracted from EMR templates, or obtained from Google  search engine. The POIs were evaluated by a panel of surgeons using the Patient  Education Materials Assessment Tool (PEMAT-P). RESULTS: Average PEMAT-P  understandability scores across all 3 sources ranged from 55% to 94%.  Actionability scores ranged from 33% to 94%, and procedure-specific scores ranged  from 50% to 92%. ChatGPT-generated POIs ranged from 82% to 88% for  understandability, 60% to 80% for actionability, and 50% to 75% for  procedure-specific items. CONCLUSIONS: ChatGPT has great potential to be a useful  adjunct in providing post-operative instructions for patients undergoing facial  trauma procedures. Detailed and patient-specific prompts are necessary to output  tailored instructions that are accurate, understandable, and actionable."
2024,Automated classification of online reviews of otolaryngologists.,Laryngoscope investigative otolaryngology,"OBJECTIVES: The study aimed to extract online comments of otolaryngologists in the 20 most populated cities in the United States from healthgrades.com, develop  and validate a natural language processing (NLP) logistic regression algorithm  for automated text classification of reviews into 10 categories, and compare 1-  and 5-star reviews in directly-physician-related and non-physician-related  categories. METHODS: 1977 1-star and 12,682 5-star reviews were collected. The  primary investigator manually categorized a training dataset of 324 1-star and  909 5-star reviews, while a validation subset of 100 5-star and 50 1-star reviews  underwent dual manual categorization. Using scikit-learn, an NLP algorithm was  trained and validated on the subsets, with F1 scores evaluating text  classification accuracy against manual categorization. The algorithm was then  applied to the entire dataset with comparison of review categorization according  to 1- and 5-star reviews. RESULTS: F1 scores for NLP validation ranged between  0.71 and 0.97. Significant associations emerged between 1-star reviews and  treatment plan, accessibility, wait time, office scheduling, billing, and  facilities. 5-star reviews were associated with surgery/procedure, bedside  manner, and staff/mid-levels. CONCLUSION: The study successfully validated an NLP  text classification system for categorizing online physician reviews. Positive  reviews were found to have an association with directly-physician related  context. 1-star reviews were related to treatment plan, accessibility, wait time,  office scheduling, billing, and facilities. This method of text classification  effectively discerned the nuances of human-written text, providing valuable  insights into online healthcare feedback that is scalable.Level of evidence:  Level 3."
2025,Utilization of Artificial Intelligence in the Creation of Patient Information on Laryngology Topics.,The Laryngoscope,"OBJECTIVE: To evaluate and compare the readability and quality of patient information generated by Chat-Generative Pre-Trained Transformer-3.5 (ChatGPT)  and the American Academy of Otolaryngology-Head and Neck Surgery (AAO-HNS) using  validated instruments including Flesch-Kincaid Grade Level (FKGL), Flesch Reading  Ease, DISCERN, and Patient Education Materials Assessment Tool for Printable  Materials (PEMAT-P). METHODS: ENTHealth.org and ChatGPT-3.5 were queried for  patient information on laryngology topics. ChatGPT-3.5 was queried twice for a  given topic to evaluate for reliability. This generated three de-identified text  documents for each topic: one from AAO-HNS and two from ChatGPT (ChatGPT Output  1, ChatGPT Output 2). Grade level and reading ease were compared between the  three sources using a one-way analysis of variance and Tukey's post hoc test.  Independent t-tests were used to compare DISCERN and PEMAT understandability and  actionability scores between AAO-HNS and ChatGPT Output 1. RESULTS: Material  generated from ChatGPT Output 1 and ChatGPT Output 2 were at least two reading  grade levels higher than that of material from AAO-HNS (p < 0.001). Regarding  reading ease, ChatGPT Output 1 and ChatGPT Output 2 documents had significantly  lower mean scores compared to AAO-HNS (p < 0.001). Moreover, ChatGPT Output 1  material on vocal cord paralysis had a lower PEMAT-P understandability compared  to that of AAO-HNS material (p > 0.05). CONCLUSION: Patient information on the  ENTHealth.org website for select laryngology topics was, on average, of a lower  grade level and higher reading ease compared to that produced by ChatGPT, but  interestingly with largely no difference in the quality of information provided.  LEVEL OF EVIDENCE: NA Laryngoscope, 135:1295-1300, 2025."
2025,Large Language Model Versus Human-Generated Thematic Analysis in Otolaryngology Qualitative Research.,The Laryngoscope,
2024,Utilizing Artificial Intelligence to Increase the Readability of Patient Education Materials in Pediatric Otolaryngology.,"Ear, nose, & throat journal","Objectives: To identify the reading levels of existing patient education materials in pediatric otolaryngology and to utilize natural language processing  artificial intelligence (AI) to reduce the reading level of patient education  materials. Methods: Patient education materials for pediatric conditions were  identified from the American Academy of Otolaryngology-Head and Neck Surgery  (AAO-HNS) website. Patient education materials about the same conditions, if  available, were identified and selected from the websites of 7 children's  hospitals. The readability of the patient materials was scored before and after  using AI with the Flesch-Kincaid calculator. ChatGPT version 3.5 was used to  convert the materials to a fifth-grade reading level. Results: On average,  AAO-HNS pediatric education material was written at a 10.71 ± 0.71 grade level.  After requesting the reduction of those materials to a fifth-grade reading level,  ChatGPT converted the same materials to an average grade level of 7.9 ± 1.18 (P <  .01). When comparing the published materials from AAO-HNS and the 7 institutions,  the average grade level was 9.32 ± 1.82, and ChatGPT was able to reduce the  average level to 7.68 ± 1.12 (P = .0598). Of the 7 children's hospitals, only 1  institution had an average grade level below the recommended sixth-grade level.  Conclusions: Patient education materials in pediatric otolaryngology were  consistently above recommended reading levels. In its current state, AI can  reduce the reading levels of education materials. However, it did not possess the  capability to reduce all materials to be below the recommended reading level."
2024,"ChatGPT Generated Otorhinolaryngology Multiple-Choice Questions: Quality, Psychometric Properties, and Suitability for Assessments.",OTO open,"OBJECTIVE: To explore Chat Generative Pretrained Transformer's (ChatGPT's) capability to create multiple-choice questions about otorhinolaryngology (ORL).  STUDY DESIGN: Experimental question generation and exam simulation. SETTING:  Tertiary academic center. METHODS: ChatGPT 3.5 was prompted: ""Can you please  create a challenging 20-question multiple-choice questionnaire about clinical  cases in otolaryngology, offering five answer options?."" The generated  questionnaire was sent to medical students, residents, and consultants. Questions  were investigated regarding quality criteria. Answers were anonymized and the  resulting data was analyzed in terms of difficulty and internal consistency.  RESULTS: ChatGPT 3.5 generated 20 exam questions of which 1 question was  considered off-topic, 3 questions had a false answer, and 3 questions had  multiple correct answers. Subspecialty theme repartition was as follows: 5  questions were on otology, 5 about rhinology, and 10 questions addressed head and  neck. The qualities of focus and relevance were good while the vignette and  distractor qualities were low. The level of difficulty was suitable for  undergraduate medical students (n = 24), but too easy for residents (n = 30) or  consultants (n = 10) in ORL. Cronbach's α was highest (.69) with 15 selected  questions using students' results. CONCLUSION: ChatGPT 3.5 is able to generate  grammatically correct simple ORL multiple choice questions for a medical student  level. However, the overall quality of the questions was average, needing  thorough review and revision by a medical expert to ensure suitability in future  exams."
2024,Assessing the Quality and Readability of Online Patient Information: ENT UK Patient Information e-Leaflets versus Responses by a Generative Artificial  Intelligence.,Facial plastic surgery : FPS,"BACKGROUND:  The evolution of artificial intelligence has introduced new ways to disseminate health information, including natural language processing models like  ChatGPT. However, the quality and readability of such digitally generated  information remains understudied. This study is the first to compare the quality  and readability of digitally generated health information against leaflets  produced by professionals. METHODOLOGY:  Patient information leaflets from five  ENT UK leaflets and their corresponding ChatGPT responses were extracted from the  Internet. Assessors with various degrees of medical knowledge evaluated the  content using the Ensuring Quality Information for Patients (EQIP) tool and  readability tools including the Flesch-Kincaid Grade Level (FKGL). Statistical  analysis was performed to identify differences between leaflets, assessors, and  sources of information. RESULTS:  ENT UK leaflets were of moderate quality,  scoring a median EQIP of 23. Statistically significant differences in overall  EQIP score were identified between ENT UK leaflets, but ChatGPT responses were of  uniform quality. Nonspecialist doctors rated the highest EQIP scores, while  medical students scored the lowest. The mean readability of ENT UK leaflets was  higher than ChatGPT responses. The information metrics of ENT UK leaflets were  moderate and varied between topics. Equivalent ChatGPT information provided  comparable content quality, but with reduced readability. CONCLUSION:  ChatGPT  patient information and professionally produced leaflets had comparable content,  but large language model content required a higher reading age. With the  increasing use of online health resources, this study highlights the need for a  balanced approach that considers both the quality and readability of patient  education materials."
2024,Enhancing Health Literacy: Evaluating the Readability of Patient Handouts Revised by ChatGPT's Large Language Model.,Otolaryngology--head and neck surgery : official journal of American Academy of Otolaryngology-Head and Neck Surgery,"OBJECTIVE: To use an artificial intelligence (AI)-powered large language model (LLM) to improve readability of patient handouts. STUDY DESIGN: Review of online  material modified by AI. SETTING: Academic center. METHODS: Five handout  materials obtained from the American Rhinologic Society (ARS) and the American  Academy of Facial Plastic and Reconstructive Surgery websites were assessed using  validated readability metrics. The handouts were inputted into OpenAI's ChatGPT-4  after prompting: ""Rewrite the following at a 6th-grade reading level."" The  understandability and actionability of both native and LLM-revised versions were  evaluated using the Patient Education Materials Assessment Tool (PEMAT). Results  were compared using Wilcoxon rank-sum tests. RESULTS: The mean readability scores  of the standard (ARS, American Academy of Facial Plastic and Reconstructive  Surgery) materials corresponded to ""difficult,"" with reading categories ranging  between high school and university grade levels. Conversely, the LLM-revised  handouts had an average seventh-grade reading level. LLM-revised handouts had  better readability in nearly all metrics tested: Flesch-Kincaid Reading Ease  (70.8 vs 43.9; P < .05), Gunning Fog Score (10.2 vs 14.42; P < .05), Simple  Measure of Gobbledygook (9.9 vs 13.1; P < .05), Coleman-Liau (8.8 vs 12.6;  P < .05), and Automated Readability Index (8.2 vs 10.7; P = .06). PEMAT scores  were significantly higher in the LLM-revised handouts for understandability (91  vs 74%; P < .05) with similar actionability (42 vs 34%; P = .15) when compared to  the standard materials. CONCLUSION: Patient-facing handouts can be augmented by  ChatGPT with simple prompting to tailor information with improved readability.  This study demonstrates the utility of LLMs to aid in rewriting patient handouts  and may serve as a tool to help optimize education materials. LEVEL OF EVIDENCE:  Level VI."
2024,Evaluating the Accuracy of ChatGPT in Common Patient Questions Regarding HPV+ Oropharyngeal Carcinoma.,"The Annals of otology, rhinology, and laryngology","OBJECTIVES: Large language model (LLM)-based chatbots such as ChatGPT have been publicly available and increasingly utilized by the general public since late  2022. This study sought to investigate ChatGPT responses to common patient  questions regarding Human Papilloma Virus (HPV) positive oropharyngeal cancer  (OPC). METHODS: This was a prospective, multi-institutional study, with data  collected from high volume institutions that perform >50 transoral robotic  surgery cases per year. The 100 most recent discussion threads including the term  ""HPV"" on the American Cancer Society's Cancer Survivors Network's Head and Neck  Cancer public discussion board were reviewed. The 11 most common questions were  serially queried to ChatGPT 3.5; answers were recorded. A survey was distributed  to fellowship trained head and neck oncologic surgeons at 3 institutions to  evaluate the responses. RESULTS: A total of 8 surgeons participated in the study.  For questions regarding HPV contraction and transmission, ChatGPT answers were  scored as clinically accurate and aligned with consensus in the head and neck  surgical oncology community 84.4% and 90.6% of the time, respectively. For  questions involving treatment of HPV+ OPC, ChatGPT was clinically accurate and  aligned with consensus 87.5% and 91.7% of the time, respectively. For questions  regarding the HPV vaccine, ChatGPT was clinically accurate and aligned with  consensus 62.5% and 75% of the time, respectively. When asked about circulating  tumor DNA testing, only 12.5% of surgeons thought responses were accurate or  consistent with consensus. CONCLUSION: ChatGPT 3.5 performed poorly with  questions involving evolving therapies and diagnostics-thus, caution should be  used when using a platform like ChatGPT 3.5 to assess use of advanced technology.  Patients should be counseled on the importance of consulting their surgeons to  receive accurate and up to date recommendations, and use LLM's to augment their  understanding of these important health-related topics."
2024,Gender Differences in Letters of Recommendations and Personal Statements for Neurotology Fellowship over 10 Years: A Deep Learning Linguistic Analysis.,"Otology & neurotology : official publication of the American Otological Society, American Neurotology Society [and] European Academy of Otology and Neurotology","OBJECTIVE: Personal statements (PSs) and letters of recommendation (LORs) are critical components of the neurotology fellowship application process but can be  subject to implicit biases. This study evaluated general and deep learning  linguistic differences between the applicant genders over a 10-year span. STUDY  DESIGN: Retrospective cohort. SETTING: Two institutions. MAIN OUTCOME MEASURES:  PSs and LORs were collected from 2014 to 2023 from two institutions. The Valence  Aware Dictionary and Sentiment Reasoner (VADER) natural language processing (NLP)  package was used to compare the positive or negative sentiment in LORs and PSs.  Next, the deep learning tool, Empath, categorized the text into scores, and  Wilcoxon rank sum tests were performed for comparisons between applicant gender.  RESULTS: Among 177 applicants over 10 years, 120 were males and 57 were females.  There were no differences in word count or VADER sentiment scores between genders  for both LORs and PSs. However, among Empath sentiment categories, male  applicants had more words of trust ( p = 0.03) and leadership ( p = 0.002) in  LORs. Temporally, the trends show a consistently higher VADER sentiment and  Empath ""trust"" and ""leader"" in male LORs from 2014 to 2019, after which there was  no statistical significance in sentiment scores between genders, and females even  have higher scores of trust and leadership in 2023. CONCLUSIONS: Linguistic  content overall favored male applicants because they were more frequently  described as trustworthy and leaders. However, the temporal analysis of  linguistic differences between male and female applicants found an encouraging  trend suggesting a reduction of gender bias in recent years, mirroring an  increased composition of women in neurotology over time."
2024,Gender-based linguistic differences in letters of recommendation for rhinology fellowship over time: A dual-institutional follow-up study using natural language  processing and deep learning.,International forum of allergy & rhinology,"This follow-up dual-institutional and longitudinal study further evaluated for underlying gender biases in LORs for rhinology fellowship. Explicit and implicit  linguistic gender bias was found, heavily favoring male applicants."
2024,"ChatGPT as a New Tool to Select a Biological for Chronic Rhino Sinusitis with Polyps, ""Caution Advised"" or ""Distant Reality""?",Journal of personalized medicine,"ChatGPT is an advanced language model developed by OpenAI, designed for natural language understanding and generation. It employs deep learning technology to  comprehend and generate human-like text, making it versatile for various  applications. The aim of this study is to assess the alignment between the  Rhinology Board's indications and ChatGPT's recommendations for treating patients  with chronic rhinosinusitis with nasal polyps (CRSwNP) using biologic therapy. An  observational cohort study involving 72 patients was conducted to evaluate  various parameters of type 2 inflammation and assess the concordance in therapy  choices between ChatGPT and the Rhinology Board. The observed results highlight  the potential of Chat-GPT in guiding optimal biological therapy selection, with a  concordance percentage = 68% and a Kappa coefficient = 0.69 (CI95% [0.50; 0.75]).  In particular, the concordance was, respectively, 79.6% for dupilumab, 20% for  mepolizumab, and 0% for omalizumab. This research represents a significant  advancement in managing CRSwNP, addressing a condition lacking robust biomarkers.  It provides valuable insights into the potential of AI, specifically ChatGPT, to  assist otolaryngologists in determining the optimal biological therapy for  personalized patient care. Our results demonstrate the need to implement the use  of this tool to effectively aid clinicians."
2024,ChatENT: Augmented Large Language Model for Expert Knowledge Retrieval in Otolaryngology-Head and Neck Surgery.,Otolaryngology--head and neck surgery : official journal of American Academy of Otolaryngology-Head and Neck Surgery,"OBJECTIVE: The recent surge in popularity of large language models (LLMs), such as ChatGPT, has showcased their proficiency in medical examinations and potential  applications in health care. However, LLMs possess inherent limitations,  including inconsistent accuracy, specific prompting requirements, and the risk of  generating harmful hallucinations. A domain-specific model might address these  limitations effectively. STUDY DESIGN: Developmental design. SETTING: Virtual.  METHODS: Otolaryngology-head and neck surgery (OHNS) relevant data were  systematically gathered from open-access Internet sources and indexed into a  knowledge database. We leveraged Retrieval-Augmented Language Modeling to recall  this information and utilized it for pretraining, which was then integrated into  ChatGPT4.0, creating an OHNS-specific knowledge question & answer platform known  as ChatENT. The model is further tested on different types of questions. RESULTS:  ChatENT showed enhanced performance in the analysis and interpretation of OHNS  information, outperforming ChatGPT4.0 in both the Canadian Royal College OHNS  sample examination questions challenge and the US board practice questions  challenge, with a 58.4% and 26.0% error reduction, respectively. ChatENT  generated fewer hallucinations and demonstrated greater consistency. CONCLUSION:  To the best of our knowledge, ChatENT is the first specialty-specific knowledge  retrieval artificial intelligence in the medical field that utilizes the latest  LLM. It appears to have considerable promise in areas such as medical education,  patient education, and clinical decision support. The model has demonstrated the  capacity to overcome the limitations of existing LLMs, thereby signaling a future  of more precise, safe, and user-friendly applications in the realm of OHNS and  other medical fields."
2024,Assessing the role of advanced artificial intelligence as a tool in multidisciplinary tumor board decision-making for primary head and neck cancer  cases.,Frontiers in oncology,"BACKGROUND: Head and neck squamous cell carcinoma (HNSCC) is a complex malignancy that requires a multidisciplinary approach in clinical practice, especially in  tumor board discussions. In recent years, artificial intelligence has emerged as  a tool to assist healthcare professionals in making informed decisions. This  study investigates the application of ChatGPT 3.5 and ChatGPT 4.0, natural  language processing models, in tumor board decision-making. METHODS: We conducted  a pilot study in October 2023 on 20 consecutive head and neck cancer patients  discussed in our multidisciplinary tumor board (MDT). Patients with a primary  diagnosis of head and neck cancer were included. The MDT and ChatGPT 3.5 and  ChatGPT 4.0 recommendations for each patient were compared by two independent  reviewers and the number of therapy options, the clinical recommendation, the  explanation and the summarization were graded. RESULTS: In this study, ChatGPT  3.5 provided mostly general answers for surgery, chemotherapy, and radiation  therapy. For clinical recommendation, explanation and summarization ChatGPT 3.5  and 4.0 scored well, but demonstrated to be mostly an assisting tool, suggesting  significantly more therapy options than our MDT, while some of the recommended  treatment modalities like primary immunotherapy are not part of the current  treatment guidelines. CONCLUSIONS: This research demonstrates that advanced AI  models at the moment can merely assist in the MDT setting, since the current  versions list common therapy options, but sometimes recommend incorrect treatment  options and in the case of ChatGPT 3.5 lack information on the source material."
2024,ChatGPT for Tinnitus Information and Support: Response Accuracy and Retest after Three and Six Months.,Brain sciences,"Testing of ChatGPT has recently been performed over a diverse range of topics. However, most of these assessments have been based on broad domains of knowledge.  Here, we test ChatGPT's knowledge of tinnitus, an important but specialized  aspect of audiology and otolaryngology. Testing involved evaluating ChatGPT's  answers to a defined set of 10 questions on tinnitus. Furthermore, given the  technology is advancing quickly, we re-evaluated the responses to the same 10  questions 3 and 6 months later. The accuracy of the responses was rated by 6  experts (the authors) using a Likert scale ranging from 1 to 5. Most of ChatGPT's  responses were rated as satisfactory or better. However, we did detect a few  instances where the responses were not accurate and might be considered somewhat  misleading. Over the first 3 months, the ratings generally improved, but there  was no more significant improvement at 6 months. In our judgment, ChatGPT  provided unexpectedly good responses, given that the questions were quite  specific. Although no potentially harmful errors were identified, some mistakes  could be seen as somewhat misleading. ChatGPT shows great potential if further  developed by experts in specific areas, but for now, it is not yet ready for  serious application."
2024,Validation of the Quality Analysis of Medical Artificial Intelligence (QAMAI) tool: a new tool to assess the quality of health information provided by AI  platforms.,European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"BACKGROUND: The widespread diffusion of Artificial Intelligence (AI) platforms is revolutionizing how health-related information is disseminated, thereby  highlighting the need for tools to evaluate the quality of such information. This  study aimed to propose and validate the Quality Assessment of Medical Artificial  Intelligence (QAMAI), a tool specifically designed to assess the quality of  health information provided by AI platforms. METHODS: The QAMAI tool has been  developed by a panel of experts following guidelines for the development of new  questionnaires. A total of 30 responses from ChatGPT4, addressing patient  queries, theoretical questions, and clinical head and neck surgery scenarios were  assessed by 27 reviewers from 25 academic centers worldwide. Construct validity,  internal consistency, inter-rater and test-retest reliability were assessed to  validate the tool. RESULTS: The validation was conducted on the basis of 792  assessments for the 30 responses given by ChatGPT4. The results of the  exploratory factor analysis revealed a unidimensional structure of the QAMAI with  a single factor comprising all the items that explained 51.1% of the variance  with factor loadings ranging from 0.449 to 0.856. Overall internal consistency  was high (Cronbach's alpha = 0.837). The Interclass Correlation Coefficient was  0.983 (95% CI 0.973-0.991; F (29,542) = 68.3; p < 0.001), indicating excellent  reliability. Test-retest reliability analysis revealed a moderate-to-strong  correlation with a Pearson's coefficient of 0.876 (95% CI 0.859-0.891;  p < 0.001). CONCLUSIONS: The QAMAI tool demonstrated significant reliability and  validity in assessing the quality of health information provided by AI platforms.  Such a tool might become particularly important/useful for physicians as patients  increasingly seek medical information on AI platforms."
2024,Machine Learning for Predictive Analysis of Otolaryngology Residency Letters of Recommendation.,The Laryngoscope,"INTRODUCTION: Letters of recommendation (LORs) are a highly influential yet subjective and often enigmatic aspect of the residency application process. This  study hypothesizes that LORs do contain valuable insights into applicants and can  be used to predict outcomes. This pilot study utilizes natural language  processing and machine learning (ML) models using LOR text to predict interview  invitations for otolaryngology residency applicants. METHODS: A total of 1642  LORs from the 2022-2023 application cycle were retrospectively retrieved from a  single institution. LORs were preprocessed and vectorized using three different  techniques to represent the text in a way that an ML model can understand written  prose: CountVectorizer (CV), Term Frequency-Inverse Document Frequency (TF-IDF),  and Word2Vec (WV). Then, the LORs were trained and tested on five ML models:  Logistic Regression (LR), Naive Bayes (NB), Decision Tree (DT), Random Forest  (RF), and Support Vector Machine (SVM). RESULTS: Of the 337 applicants, 67 were  interviewed and 270 were not interviewed. In total, 1642 LORs (26.7% interviewed)  were analyzed. The two best-performing ML models in predicting interview  invitations were the TF-IDF vectorized DT and CV vectorized DT models.  CONCLUSION: This preliminary study revealed that ML models and vectorization  combinations can provide better-than-chance predictions for interview invitations  for otolaryngology residency applicants. The high-performing ML models were able  to classify meaningful information from the LORs to predict applicant interview  invitation. The potential of an automated process to help predict an applicant's  likelihood of obtaining an interview invitation could be a valuable tool for  training programs in the future. LEVEL OF EVIDENCE: N/A Laryngoscope,  134:4016-4022, 2024."
2024,Evaluating the Potential of AI Chatbots in Treatment Decision-making for Acquired Bilateral Vocal Fold Paralysis in Adults.,Journal of voice : official journal of the Voice Foundation,"OBJECTIVES: The development of artificial intelligence-powered language models, such as Chatbot Generative Pre-trained Transformer (ChatGPT) or Large Language  Model Meta AI (Llama), is emerging in medicine. Patients and practitioners have  full access to chatbots that may provide medical information. The aim of this  study was to explore the performance and accuracy of ChatGPT and Llama in  treatment decision-making for bilateral vocal fold paralysis (BVFP). METHODS:  Data of 20 clinical cases, treated between 2018 and 2023, were retrospectively  collected from four tertiary laryngology centers in Europe. The cases were  defined as the most common or most challenging scenarios regarding BVFP  treatment. The treatment proposals were discussed in their local  multidisciplinary teams (MDT). Each case was presented to ChatGPT-4.0 and Llama  Chat-2.0, and potential treatment strategies were requested. The Artificial  Intelligence Performance Instrument (AIPI) treatment subscore was used to compare  both Chatbots' performances to MDT treatment proposal. RESULTS: Most common  etiology of BVFP was thyroid surgery. A form of partial arytenoidectomy with or  without posterior transverse cordotomy was the MDT proposal for most cases. The  accuracy of both Chatbots was very low regarding their treatment proposals, with  a maximum AIPI treatment score in 5% of the cases. In most cases even harmful  assertions were made, including the suggestion of vocal fold medialisation to  treat patients with stridor and dyspnea. ChatGPT-4.0 performed significantly  better in suggesting the correct treatment as part of the treatment proposal  (50%) compared to Llama Chat-2.0 (15%). CONCLUSION: ChatGPT and Llama are judged  as inaccurate in proposing correct treatment for BVFP. ChatGPT significantly  outperformed Llama. Treatment decision-making for a complex condition such as  BVFP is clearly beyond the Chatbot's knowledge expertise. This study highlights  the complexity and heterogeneity of BVFP treatment, and the need for further  guidelines dedicated to the management of BVFP."
2024,Moving Biosurveillance Beyond Coded Data Using AI for Symptom Detection From Physician Notes: Retrospective Cohort Study.,Journal of medical Internet research,"BACKGROUND: Real-time surveillance of emerging infectious diseases necessitates a dynamically evolving, computable case definition, which frequently incorporates  symptom-related criteria. For symptom detection, both population health  monitoring platforms and research initiatives primarily depend on structured data  extracted from electronic health records. OBJECTIVE: This study sought to  validate and test an artificial intelligence (AI)-based natural language  processing (NLP) pipeline for detecting COVID-19 symptoms from physician notes in  pediatric patients. We specifically study patients presenting to the emergency  department (ED) who can be sentinel cases in an outbreak. METHODS: Subjects in  this retrospective cohort study are patients who are 21 years of age and younger,  who presented to a pediatric ED at a large academic children's hospital between  March 1, 2020, and May 31, 2022. The ED notes for all patients were processed  with an NLP pipeline tuned to detect the mention of 11 COVID-19 symptoms based on  Centers for Disease Control and Prevention (CDC) criteria. For a gold standard, 3  subject matter experts labeled 226 ED notes and had strong agreement  (F(1)-score=0.986; positive predictive value [PPV]=0.972; and sensitivity=1.0).  F(1)-score, PPV, and sensitivity were used to compare the performance of both NLP  and the International Classification of Diseases, 10th Revision (ICD-10) coding  to the gold standard chart review. As a formative use case, variations in symptom  patterns were measured across SARS-CoV-2 variant eras. RESULTS: There were 85,678  ED encounters during the study period, including 4% (n=3420) with patients with  COVID-19. NLP was more accurate at identifying encounters with patients that had  any of the COVID-19 symptoms (F(1)-score=0.796) than ICD-10 codes (F(1)-score  =0.451). NLP accuracy was higher for positive symptoms (sensitivity=0.930) than  ICD-10 (sensitivity=0.300). However, ICD-10 accuracy was higher for negative  symptoms (specificity=0.994) than NLP (specificity=0.917). Congestion or runny  nose showed the highest accuracy difference (NLP: F(1)-score=0.828 and ICD-10:  F(1)-score=0.042). For encounters with patients with COVID-19, prevalence  estimates of each NLP symptom differed across variant eras. Patients with  COVID-19 were more likely to have each NLP symptom detected than patients without  this disease. Effect sizes (odds ratios) varied across pandemic eras.  CONCLUSIONS: This study establishes the value of AI-based NLP as a highly  effective tool for real-time COVID-19 symptom detection in pediatric patients,  outperforming traditional ICD-10 methods. It also reveals the evolving nature of  symptom prevalence across different virus variants, underscoring the need for  dynamic, technology-driven approaches in infectious disease surveillance."
2024,Performance of GPT-4V in Answering the Japanese Otolaryngology Board Certification Examination Questions: Evaluation Study.,JMIR medical education,"BACKGROUND: Artificial intelligence models can learn from medical literature and clinical cases and generate answers that rival human experts. However, challenges  remain in the analysis of complex data containing images and diagrams. OBJECTIVE:  This study aims to assess the answering capabilities and accuracy of ChatGPT-4  Vision (GPT-4V) for a set of 100 questions, including image-based questions, from  the 2023 otolaryngology board certification examination. METHODS: Answers to 100  questions from the 2023 otolaryngology board certification examination, including  image-based questions, were generated using GPT-4V. The accuracy rate was  evaluated using different prompts, and the presence of images, clinical area of  the questions, and variations in the answer content were examined. RESULTS: The  accuracy rate for text-only input was, on average, 24.7% but improved to 47.3%  with the addition of English translation and prompts (P<.001). The average  nonresponse rate for text-only input was 46.3%; this decreased to 2.7% with the  addition of English translation and prompts (P<.001). The accuracy rate was lower  for image-based questions than for text-only questions across all types of input,  with a relatively high nonresponse rate. General questions and questions from the  fields of head and neck allergies and nasal allergies had relatively high  accuracy rates, which increased with the addition of translation and prompts. In  terms of content, questions related to anatomy had the highest accuracy rate. For  all content types, the addition of translation and prompts increased the accuracy  rate. As for the performance based on image-based questions, the average of  correct answer rate with text-only input was 30.4%, and that with text-plus-image  input was 41.3% (P=.02). CONCLUSIONS: Examination of artificial intelligence's  answering capabilities for the otolaryngology board certification examination  improves our understanding of its potential and limitations in this field.  Although the improvement was noted with the addition of translation and prompts,  the accuracy rate for image-based questions was lower than that for text-based  questions, suggesting room for improvement in GPT-4V at this stage. Furthermore,  text-plus-image input answers a higher rate in image-based questions. Our  findings imply the usefulness and potential of GPT-4V in medicine; however,  future consideration of safe use methods is needed."
2024,ChatGPT-4 accuracy for patient education in laryngopharyngeal reflux.,European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"INTRODUCTION: Chatbot Generative Pre-trained Transformer (ChatGPT) is an artificial intelligence-powered language model chatbot able to help  otolaryngologists in practice and research. The ability of ChatGPT in generating  patient-centered information related to laryngopharyngeal reflux disease (LPRD)  was evaluated. METHODS: Twenty-five questions dedicated to definition, clinical  presentation, diagnosis, and treatment of LPRD were developed from the Dubai  definition and management of LPRD consensus and recent reviews. Questions about  the four aforementioned categories were entered into ChatGPT-4. Four  board-certified laryngologists evaluated the accuracy of ChatGPT-4 with a 5-point  Likert scale. Interrater reliability was evaluated. RESULTS: The mean scores (SD)  of ChatGPT-4 answers for definition, clinical presentation, additional  examination, and treatments were 4.13 (0.52), 4.50 (0.72), 3.75 (0.61), and 4.18  (0.47), respectively. Experts reported high interrater reliability for sub-scores  (ICC = 0.973). The lowest performances of ChatGPT-4 were on answers about the  most prevalent LPR signs, the most reliable objective tool for the diagnosis  (hypopharyngeal-esophageal multichannel intraluminal impedance-pH monitoring  (HEMII-pH)), and the criteria for the diagnosis of LPR using HEMII-pH.  CONCLUSION: ChatGPT-4 may provide adequate information on the definition of LPR,  differences compared to GERD (gastroesophageal reflux disease), and clinical  presentation. Information provided upon extra-laryngeal manifestations and  HEMII-pH may need further optimization. Regarding the recent trends identifying  increasing patient use of internet sources for self-education, the findings of  the present study may help draw attention to ChatGPT-4's accuracy on the topic of  LPR."
2024,The utility and accuracy of ChatGPT in providing post-operative instructions following tonsillectomy: A pilot study.,International journal of pediatric otorhinolaryngology,"OBJECTIVE: To investigate the utility of answers generated by ChatGPT, a large language model, to common questions parents have for their children following  tonsillectomy. METHODS: Twenty Otolaryngology residents anonymously submitted  common questions asked by parents of pediatric patients following tonsillectomy.  After identifying the 16 most common questions via consensus-based approach, we  asked ChatGPT to generate responses to these queries. Satisfaction with the  AI-generated answers was rated from 1 (Worst) to 5 (Best) by an expert panel of 3  pediatric Otolaryngologists. RESULTS: The distribution of questions across the  five most common domains, their mean satisfaction scores, and their Krippendorf's  interrater reliability coefficient were: Pain management [6, (3.67), (0.434)],  Complications [4, (3.58), (-0.267)], Diet [3, (4.33), (-0.357)], Physical  Activity [2, (4.33), (-0.318)], and Follow-up [1, (2.67), (-0.250)]. The panel  noted that answers for diet, bleeding complications, and return to school were  thorough. Pain management and follow-up recommendations were inaccurate,  including a recommendation to prescribe codeine to children despite a black-box  warning, and a suggested post-operative follow-up at 1 week, rather than the  customary 2-4 weeks for our panel. CONCLUSION: Although ChatGPT can provide  accurate answers for common patient questions following tonsillectomy, it  sometimes provides eloquently written inaccurate information. This may lead to  patients using AI-generated medical advice contrary to physician advice. The  inaccuracy in pain management answers likely reflects regional practice  variability. If trained appropriately, ChatGPT could be an excellent resource for  Otolaryngologists and patients to answer questions in the postoperative period.  Future research should investigate if Otolaryngologist-trained models can  increase the accuracy of responses."
2024,ChatGPT as an information tool in rhinology. Can we trust each other today?,European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"PURPOSE: ChatGPT (Chat-Generative Pre-trained Transformer) has proven to be a powerful information tool on various topics, including healthcare. This system is  based on information obtained on the Internet, but this information is not always  reliable. Currently, few studies analyze the validity of these responses in  rhinology. Our work aims to assess the quality and reliability of the information  provided by AI regarding the main rhinological pathologies. METHODS: We asked to  the default ChatGPT version (GPT-3.5) 65 questions about the most prevalent  pathologies in rhinology. The focus was learning about the causes, risk factors,  treatments, prognosis, and outcomes. We use the Discern questionnaire and a  hexagonal radar schema to evaluate the quality of the information. We use  Fleiss's kappa statistical analysis to determine the consistency of agreement  between different observers. RESULTS: The overall evaluation of the Discern  questionnaire resulted in a score of 4.05 (± 0.6). The results in the Reliability  section are worse, with an average score of 3.18. (± 1.77). This score is  affected by the responses to questions about the source of the information  provided. The average score for the Quality section was 3.59 (± 1.18). Fleiss's  Kappa shows substantial agreement, with a K of 0.69 (p < 0.001). CONCLUSION: The  ChatGPT answers are accurate and reliable. It generates a simple and  understandable description of the pathology for the patient's benefit. Our team  considers that ChatGPT could be a useful tool to provide information under prior  supervision by a health professional."
2024,Exploring the landscape of AI-assisted decision-making in head and neck cancer treatment: a comparative analysis of NCCN guidelines and ChatGPT responses.,European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"PURPOSE: Recent breakthroughs in natural language processing and machine learning, exemplified by ChatGPT, have spurred a paradigm shift in healthcare.  Released by OpenAI in November 2022, ChatGPT rapidly gained global attention.  Trained on massive text datasets, this large language model holds immense  potential to revolutionize healthcare. However, existing literature often  overlooks the need for rigorous validation and real-world applicability. METHODS:  This head-to-head comparative study assesses ChatGPT's capabilities in providing  therapeutic recommendations for head and neck cancers. Simulating every NCCN  Guidelines scenarios. ChatGPT is queried on primary treatments, adjuvant  treatment, and follow-up, with responses compared to the NCCN Guidelines.  Performance metrics, including sensitivity, specificity, and F1 score, are  employed for assessment. RESULTS: The study includes 68 hypothetical cases and  204 clinical scenarios. ChatGPT exhibits promising capabilities in addressing  NCCN-related queries, achieving high sensitivity and overall accuracy across  primary treatment, adjuvant treatment, and follow-up. The study's metrics  showcase robustness in providing relevant suggestions. However, a few  inaccuracies are noted, especially in primary treatment scenarios. CONCLUSION:  Our study highlights the proficiency of ChatGPT in providing treatment  suggestions. The model's alignment with the NCCN Guidelines sets the stage for a  nuanced exploration of AI's evolving role in oncological decision support.  However, challenges related to the interpretability of AI in clinical  decision-making and the importance of clinicians understanding the underlying  principles of AI models remain unexplored. As AI continues to advance,  collaborative efforts between models and medical experts are deemed essential for  unlocking new frontiers in personalized cancer care."
2024,ChatGPT vs. web search for patient questions: what does ChatGPT do better?,European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"PURPOSE: Chat generative pretrained transformer (ChatGPT) has the potential to significantly impact how patients acquire medical information online. Here, we  characterize the readability and appropriateness of ChatGPT responses to a range  of patient questions compared to results from traditional web searches. METHODS:  Patient questions related to the published Clinical Practice Guidelines by the  American Academy of Otolaryngology-Head and Neck Surgery were sourced from  existing online posts. Questions were categorized using a modified Rothwell  classification system into (1) fact, (2) policy, and (3) diagnosis and  recommendations. These were queried using ChatGPT and traditional web search. All  results were evaluated on readability (Flesch Reading Ease and Flesch-Kinkaid  Grade Level) and understandability (Patient Education Materials Assessment Tool).  Accuracy was assessed by two blinded clinical evaluators using a three-point  ordinal scale. RESULTS: 54 questions were organized into fact (37.0%), policy  (37.0%), and diagnosis (25.8%). The average readability for ChatGPT responses was  lower than traditional web search (FRE: 42.3 ± 13.1 vs. 55.6 ± 10.5, p < 0.001),  while the PEMAT understandability was equivalent (93.8% vs. 93.5%, p = 0.17).  ChatGPT scored higher than web search for questions the 'Diagnosis' category  (p < 0.01); there was no difference in questions categorized as 'Fact' (p = 0.15)  or 'Policy' (p = 0.22). Additional prompting improved ChatGPT response  readability (FRE 55.6 ± 13.6, p < 0.01). CONCLUSIONS: ChatGPT outperforms web  search in answering patient questions related to symptom-based diagnoses and is  equivalent in providing medical facts and established policy. Appropriate  prompting can further improve readability while maintaining accuracy. Further  patient education is needed to relay the benefits and limitations of this  technology as a source of medial information."
2024,An introduction to machine learning and generative artificial intelligence for otolaryngologists-head and neck surgeons: a narrative review.,European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"PURPOSE: Despite the robust expansion of research surrounding artificial intelligence (AI) and machine learning (ML) and their applications to medicine,  these methodologies often remain opaque and inaccessible to many  otolaryngologists. Especially, with the increasing ubiquity of large-language  models (LLMs), such as ChatGPT and their potential implementation in clinical  practice, clinicians may benefit from a baseline understanding of some aspects of  AI. In this narrative review, we seek to clarify underlying concepts, illustrate  applications to otolaryngology, and highlight future directions and limitations  of these tools. METHODS: Recent literature regarding AI principles and  otolaryngologic applications of ML and LLMs was reviewed via search in PubMed and  Google Scholar. RESULTS: Significant recent strides have been made in  otolaryngology research utilizing AI and ML, across all subspecialties, including  neurotology, head and neck oncology, laryngology, rhinology, and sleep surgery.  Potential applications suggested by recent publications include screening and  diagnosis, predictive tools, clinical decision support, and clinical workflow  improvement via LLMs. Ongoing concerns regarding AI in medicine include ethical  concerns around bias and data sharing, as well as the ""black box"" problem and  limitations in explainability. CONCLUSIONS: Potential implementations of AI in  otolaryngology are rapidly expanding. While implementation in clinical practice  remains theoretical for most of these tools, their potential power to influence  the practice of otolaryngology is substantial."
2024,"A quantitative analysis of Twitter (""X"") trends in the discussion of rhinoplasty.",Laryngoscope investigative otolaryngology,"INTRODUCTION: Rhinoplasty is one of the most common cosmetic surgical procedures performed globally. Twitter, also known as ""X,"" is used by both patients and  physicians and has been studied as a useful tool for analyzing trends in  healthcare. The public social media discourse of rhinoplasty has not been  previously reported in the field of otolaryngology. The goal of this study was to  characterize the most common user type, sentiment, and temporal trends in the  discussion of rhinoplasty on Twitter to guide facial plastic surgeons in their  clinical and social media practices. METHODS: A total of 1,427,015 tweets  published from 2015 to 2020 containing the keywords ""rhinoplasty"" or ""nose job""  were extracted using Twitter Academic API. Tweets were standardized and filtered  for spam and duplication. Natural language processing (NLP) algorithms and data  visualization techniques were applied to characterize tweets. RESULTS:  Significantly more ""nose job"" tweets (80.8%) were published compared with  ""rhinoplasty"" (19.2%). Annual tweet frequency increased over the 5 years, with  ""rhinoplasty"" tweets peaking in January and ""nose job"" tweets peaking in the  summer and winter months. Most ""rhinoplasty"" tweets were linked to a surgeon or  medical practice source, while most ""nose job"" tweets were from isolated  laypersons. While discussion was positive in sentiment overall (M = +0.08), ""nose  job"" tweets had lower average sentiment scores (P < .001) and over twice the  proportion of negative tweets. The top 20 most prolific accounts contributed to  14,758 (10.6%) of total ""rhinoplasty"" tweets. Exactly 90% (18/20) of those  accounts linked to non-academic surgeons compared with 10% (2/20) linked to  academic surgeons. CONCLUSIONS: Rhinoplasty-related posts on Twitter were  cumulatively positive in sentiment and tweet volume is steadily increasing over  time, especially during popular holiday months. The search term ""nose job"" yields  significantly more results than ""rhinoplasty,"" and is the preferred term of  non-healthcare users. We found a large digital contribution from surgeons and  medical practices, particularly in the non-academic and private practice sector,  utilizing Twitter for promotional purposes."
2024,Can ChatGPT help patients answer their otolaryngology questions?,Laryngoscope investigative otolaryngology,"BACKGROUND: Over the past year, the world has been captivated by the potential of artificial intelligence (AI). The appetite for AI in science, specifically  healthcare is huge. It is imperative to understand the credibility of large  language models in assisting the public in medical queries. OBJECTIVE: To  evaluate the ability of ChatGPT to provide reasonably accurate answers to public  queries within the domain of Otolaryngology. METHODS: Two board-certified  otolaryngologists (HZ, RS) inputted 30 text-based patient queries into the  ChatGPT-3.5 model. ChatGPT responses were rated by physicians on a scale  (accurate, partially accurate, incorrect), while a similar 3-point scale  involving confidence was given to layperson reviewers. Demographic data involving  gender and education level was recorded for the public reviewers. Inter-rater  agreement percentage was based on binomial distribution for calculating the 95%  confidence intervals and performing significance tests. Statistical significance  was defined as p < .05 for two-sided tests. RESULTS: In testing patient queries,  both Otolaryngology physicians found that ChatGPT answered 98.3% of questions  correctly, but only 79.8% (range 51.7%-100%) of patients were confident that the  AI model was accurate in its responses (corrected agreement = 0.682; p < .001).  Among the layperson responses, the corrected coefficient was of moderate  agreement (0.571; p < .001). No correlation was noted among age, gender, or  education level for the layperson responses. CONCLUSION: ChatGPT is highly  accurate in responding to questions posed by the public with regards to  Otolaryngology from a physician standpoint. Public reviewers were not fully  confident in believing the AI model, with subjective concerns related to less  trust in AI answers compared to physician explanation. Larger evaluations with a  representative public sample and broader medical questions should immediately be  conducted by appropriate organizations, governing bodies, and/or governmental  agencies to instill public confidence in AI and ChatGPT as a medical resource.  LEVEL OF EVIDENCE: 4."
2024,A Novel Evaluation Model for Assessing ChatGPT on Otolaryngology-Head and Neck Surgery Certification Examinations: Performance Study.,JMIR medical education,"BACKGROUND: ChatGPT is among the most popular large language models (LLMs), exhibiting proficiency in various standardized tests, including multiple-choice  medical board examinations. However, its performance on otolaryngology-head and  neck surgery (OHNS) certification examinations and open-ended medical board  certification examinations has not been reported. OBJECTIVE: We aimed to evaluate  the performance of ChatGPT on OHNS board examinations and propose a novel method  to assess an AI model's performance on open-ended medical board examination  questions. METHODS: Twenty-one open-ended questions were adopted from the Royal  College of Physicians and Surgeons of Canada's sample examination to query  ChatGPT on April 11, 2023, with and without prompts. A new model, named  Concordance, Validity, Safety, Competency (CVSC), was developed to evaluate its  performance. RESULTS: In an open-ended question assessment, ChatGPT achieved a  passing mark (an average of 75% across 3 trials) in the attempts and demonstrated  higher accuracy with prompts. The model demonstrated high concordance (92.06%)  and satisfactory validity. While demonstrating considerable consistency in  regenerating answers, it often provided only partially correct responses.  Notably, concerning features such as hallucinations and self-conflicting answers  were observed. CONCLUSIONS: ChatGPT achieved a passing score in the sample  examination and demonstrated the potential to pass the OHNS certification  examination of the Royal College of Physicians and Surgeons of Canada. Some  concerns remain due to its hallucinations, which could pose risks to patient  safety. Further adjustments are necessary to yield safer and more accurate  answers for clinical implementation."
2024,Accuracy of ChatGPT-3.5 and -4 in providing scientific references in otolaryngology-head and neck surgery.,European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"INTRODUCTION: Chatbot generative pre-trained transformer (ChatGPT) is a new artificial intelligence-powered language model of chatbot able to help  otolaryngologists in practice and research. We investigated the accuracy of  ChatGPT-3.5 and -4 in the referencing of manuscripts published in otolaryngology.  METHODS: ChatGPT-3.5 and ChatGPT-4 were interrogated for providing references of  the top-30 most cited papers in otolaryngology in the past 40 years including  clinical guidelines and key studies that changed the practice. The responses were  regenerated three times to assess the accuracy and stability of ChatGPT.  ChatGPT-3.5 and ChatGPT-4 were compared for accuracy of reference and potential  mistakes. RESULTS: The accuracy of ChatGPT-3.5 and ChatGPT-4.0 ranged from 47% to  60%, and 73% to 87%, respectively (p < 0.005). ChatGPT-3.5 provided 19 inaccurate  references and invented 2 references throughout the regenerated questions.  ChatGPT-4.0 provided 13 inaccurate references, while it proposed only one  invented reference. The stability of responses throughout regenerated answers was  mild (k = 0.238) and moderate (k = 0.408) for ChatGPT-3.5 and 4.0, respectively.  CONCLUSIONS: ChatGPT-4.0 reported higher accuracy than the free-access version  (3.5). False references were detected in both 3.5 and 4.0 versions. Practitioners  need to be careful regarding the use of ChatGPT in the reach of some key  reference when writing a report."
2024,Reliability of large language models in managing odontogenic sinusitis clinical scenarios: a preliminary multidisciplinary evaluation.,European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"PURPOSE: This study aimed to evaluate the utility of large language model (LLM) artificial intelligence tools, Chat Generative Pre-Trained Transformer (ChatGPT)  versions 3.5 and 4, in managing complex otolaryngological clinical scenarios,  specifically for the multidisciplinary management of odontogenic sinusitis (ODS).  METHODS: A prospective, structured multidisciplinary specialist evaluation was  conducted using five ad hoc designed ODS-related clinical scenarios. LLM  responses to these scenarios were critically reviewed by a multidisciplinary  panel of eight specialist evaluators (2 ODS experts, 2 rhinologists, 2 general  otolaryngologists, and 2 maxillofacial surgeons). Based on the level of  disagreement from panel members, a Total Disagreement Score (TDS) was calculated  for each LLM response, and TDS comparisons were made between ChatGPT3.5 and  ChatGPT4, as well as between different evaluators. RESULTS: While disagreement to  some degree was demonstrated in 73/80 evaluator reviews of LLMs' responses, TDSs  were significantly lower for ChatGPT4 compared to ChatGPT3.5. Highest TDSs were  found in the case of complicated ODS with orbital abscess, presumably due to  increased case complexity with dental, rhinologic, and orbital factors affecting  diagnostic and therapeutic options. There were no statistically significant  differences in TDSs between evaluators' specialties, though ODS experts and  maxillofacial surgeons tended to assign higher TDSs. CONCLUSIONS: LLMs like  ChatGPT, especially newer versions, showed potential for complimenting  evidence-based clinical decision-making, but substantial disagreement was still  demonstrated between LLMs and clinical specialists across most case examples,  suggesting they are not yet optimal in aiding clinical management decisions.  Future studies will be important to analyze LLMs' performance as they evolve over  time."
2024,Utility of a LangChain and OpenAI GPT-powered chatbot based on the international consensus statement on allergy and rhinology: Rhinosinusitis.,International forum of allergy & rhinology,We created a LangChain/OpenAI API-powered chatbot based solely on International Consensus Statement of Allergy and Rhinology: Rhinosinusitis (ICAR-RS). The  ICAR-RS chatbot is able to provide direct and actionable recommendations.  Utilization of consensus statements provides an opportunity for AI applications  in healthcare.
2024,ChatGPT and Rhinoplasty Recovery: An Exploration of AI's Role in Postoperative Guidance.,Facial plastic surgery : FPS,"The potential applications of artificial intelligence (AI) in health care have garnered significant interest in recent years. This study presents the first  published exploration of ChatGPT, an AI language model, as a tool for providing  postoperative guidance during rhinoplasty recovery. The primary objective was to  shed light on the role of ChatGPT in augmenting patient care during the critical  postoperative phase. Using the Rhinobase database, standardized questions were  formulated to evaluate AI-generated responses addressing pain management,  swelling, bruising, and potential asymmetries. Results demonstrated that ChatGPT  has the potential to enhance patient education and alleviate emotional distress  by providing general information and reassurance during the recovery process.  However, the study emphasized that AI should not replace personalized advice from  qualified health care professionals. This pioneering investigation offers  valuable insights into the integration of AI and human expertise, paving the way  for optimized postrhinoplasty recovery care."
2023,Evaluating the Current Ability of ChatGPT to Assist in Professional Otolaryngology Education.,OTO open,"OBJECTIVE: To quantify ChatGPT's concordance with expert Otolaryngologists when posed with high-level questions that require blending rote memorization and  critical thinking. STUDY DESIGN: Cross-sectional survey. SETTING: OpenAI's  ChatGPT-3.5 Platform. METHODS: Two board-certified otolaryngologists (HZ, RS)  input 2 sets of 30 text-based questions (open-ended and single-answer  multiple-choice) into the ChatGPT-3.5 model. Responses were rated on a scale  (correct, partially correct, incorrect) by each Otolaryngologist working  simultaneously with the AI model. Interrater agreement percentage was based on  binomial distribution for calculating the 95% confidence intervals and performing  significance tests. Statistical significance was defined as P < .05 for 2-sided  tests. RESULTS: In testing open-ended questions, the ChatGPT model had 56.7% of  initially answering questions with complete accuracy, and 86.7% chance of answer  with some accuracy (corrected agreement = 80.1%; P < .001). For repeat questions,  ChatGPT improved to 73.3% with complete accuracy and 96.7% with some accuracy  (corrected agreement = 88.8%; P < .001). For multiple-choice questions, the  ChatGPT model performed substantially worse (43.3% correct). CONCLUSION: ChatGPT  currently does not provide reliably accurate responses to sophisticated questions  in Otolaryngology. Professional societies must be aware of the potential of this  tool and prevent unscrupulous use during test-taking situations and consider  guidelines for clinical scenarios. Expert clinical oversight is still necessary  for myriad use cases (eg, hallucination)."
2023,A Five-Decade Text Mining Analysis of Cochlear Implant Research: Where We Started and Where We Are Heading.,"Medicina (Kaunas, Lithuania)","Background and Objectives: Since its invention in the 1970s, the cochlear implant (CI) has been substantially developed. We aimed to assess the trends in the  published literature to characterize CI. Materials and Methods: We queried PubMed  for all CI-related entries published during 1970-2022. The following data were  extracted: year of publication, publishing journal, title, keywords, and abstract  text. Search terms belonged to the patient's age group, etiology for hearing  loss, indications for CI, and surgical methodological advancement. Annual trends  of publications were plotted. The slopes of publication trends were calculated by  fitting regression lines to the yearly number of publications. Results: Overall,  19,428 CIs articles were identified. Pediatric-related CI was the most dominant  sub-population among the age groups, with the highest rate and slope during the  years (slope 5.2 ± 0.3, p < 0.001), while elderly-related CIs had significantly  fewer publications. Entries concerning hearing preservation showed the sharpest  rise among the methods, from no entries in 1980 to 46 entries in 2021 (slope 1.7  ± 0.2, p < 0.001). Entries concerning robotic surgery emerged in 2000, with a  sharp increase in recent years (slope 0.5 ± 0.1, p < 0.001). Drug-eluting  electrodes and CI under local-anesthesia have been reported only in the past five  years, with a gradual rise. Conclusions: Publications regarding CI among  pediatrics outnumbered all other indications, supporting the rising, pivotal role  of CI in the rehabilitation of children with sensorineural hearing loss.  Hearing-preservation publications have recently rapidly risen, identified as the  primary trend of the current era, followed by a sharp rise of robotic surgery  that is evolving and could define the next revolution."
2023,Genetic Discovery Enabled by A Large Language Model.,,"Artificial intelligence (AI) has been used in many areas of medicine, and recently large language models (LLMs) have shown potential utility for clinical  applications. However, since we do not know if the use of LLMs can accelerate the  pace of genetic discovery, we used data generated from mouse genetic models to  investigate this possibility. We examined whether a recently developed  specialized LLM (Med-PaLM 2) could analyze sets of candidate genes generated from  analysis of murine models of biomedical traits. In response to free-text input,  Med-PaLM 2 correctly identified the murine genes that contained experimentally  verified causative genetic factors for six biomedical traits, which included  susceptibility to diabetes and cataracts. Med-PaLM 2 was also able to analyze a  list of genes with high impact alleles, which were identified by comparative  analysis of murine genomic sequence data, and it identified a causative murine  genetic factor for spontaneous hearing loss. Based upon this Med-PaLM 2 finding,  a novel bigenic model for susceptibility to spontaneous hearing loss was  developed. These results demonstrate Med-PaLM 2 can analyze gene-phenotype  relationships and generate novel hypotheses, which can facilitate genetic  discovery."
2024,Chat GPT for the management of obstructive sleep apnea: do we have a polar star?,European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"PURPOSE: This study explores the potential of the Chat-Generative Pre-Trained Transformer (Chat-GPT), a Large Language Model (LLM), in assisting healthcare  professionals in the diagnosis of obstructive sleep apnea (OSA). It aims to  assess the agreement between Chat-GPT's responses and those of expert  otolaryngologists, shedding light on the role of AI-generated content in medical  decision-making. METHODS: A prospective, cross-sectional study was conducted,  involving 350 otolaryngologists from 25 countries who responded to a specialized  OSA survey. Chat-GPT was tasked with providing answers to the same survey  questions. Responses were assessed by both super-experts and statistically  analyzed for agreement. RESULTS: The study revealed that Chat-GPT and expert  responses shared a common answer in over 75% of cases for individual questions.  However, the overall consensus was achieved in only four questions. Super-expert  assessments showed a moderate agreement level, with Chat-GPT scoring slightly  lower than experts. Statistically, Chat-GPT's responses differed significantly  from experts' opinions (p = 0.0009). Sub-analysis revealed areas of improvement  for Chat-GPT, particularly in questions where super-experts rated its responses  lower than expert consensus. CONCLUSIONS: Chat-GPT demonstrates potential as a  valuable resource for OSA diagnosis, especially where access to specialists is  limited. The study emphasizes the importance of AI-human collaboration, with  Chat-GPT serving as a complementary tool rather than a replacement for medical  professionals. This research contributes to the discourse in otolaryngology and  encourages further exploration of AI-driven healthcare applications. While  Chat-GPT exhibits a commendable level of consensus with expert responses, ongoing  refinements in AI-based healthcare tools hold significant promise for the future  of medicine, addressing the underdiagnosis and undertreatment of OSA and  improving patient outcomes."
2023,Artificial intelligence in medical referrals triage based on Clinical Prioritization Criteria.,Frontiers in digital health,"The clinical prioritisation criteria (CPC) are a clinical decision support tool that ensures patients referred for public specialist outpatient services to  Queensland Health are assessed according to their clinical urgency. Medical  referrals are manually triaged and prioritised into three categories by the  associated health service before appointments are booked. We have developed a  method using artificial intelligence to automate the process of categorizing  medical referrals based on clinical prioritization criteria (CPC) guidelines.  Using machine learning techniques, we have created a tool that can assist  clinicians in sorting through the substantial number of referrals they receive  each year, leading to more efficient use of clinical specialists' time and  improved access to healthcare for patients. Our research included analyzing  17,378 ENT referrals from two hospitals in Queensland between 2019 and 2022. Our  results show a level of agreement between referral categories and generated  predictions of 53.8%."
2024,"Is artificial intelligence ready to replace specialist doctors entirely? ENT specialists vs ChatGPT: 1-0, ball at the center.",European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"PURPOSE: The purpose of this study is to evaluate ChatGPT's responses to Ear, Nose and Throat (ENT) clinical cases and compare them with the responses of ENT  specialists. METHODS: We have hypothesized 10 scenarios, based on ENT daily  experience, with the same primary symptom. We have constructed 20 clinical cases,  2 for each scenario. We described them to 3 ENT specialists and ChatGPT. The  difficulty of the clinical cases was assessed by the 5 ENT authors of this  article. The responses of ChatGPT were evaluated by the 5 ENT authors of this  article for correctness and consistency with the responses of the 3 ENT experts.  To verify the stability of ChatGPT's responses, we conducted the searches, always  from the same account, for 5 consecutive days. RESULTS: Among the 20 cases, 8  were rated as low complexity, 6 as moderate complexity and 6 as high complexity.  The overall mean correctness and consistency score of ChatGPT responses was 3.80  (SD 1.02) and 2.89 (SD 1.24), respectively. We did not find a statistically  significant difference in the average ChatGPT correctness and coherence score  according to case complexity. The total intraclass correlation coefficient (ICC)  for the stability of the correctness and consistency of ChatGPT was 0.763 (95%  confidence interval [CI] 0.553-0.895) and 0.837 (95% CI 0.689-0.927),  respectively. CONCLUSIONS: Our results revealed the potential usefulness of  ChatGPT in ENT diagnosis. The instability in responses and the inability to  recognise certain clinical elements are its main limitations."
2024,Using ChatGPT to Generate Research Ideas in Dysphagia: A Pilot Study.,Dysphagia,"Current research in dysphagia faces challenges due to the rapid growth of scientific literature and the interdisciplinary nature of the field. To address  this, the study evaluates ChatGPT, an AI language model, as a supplementary  resource to assist clinicians and researchers in generating research ideas for  dysphagia, utilizing recent advancements in natural language processing and  machine learning. The research ideas were generated through ChatGPT's command to  explore diverse aspects of dysphagia. A web-based survey was conducted, 45  dysphagia experts were asked to rank each study on a scale of 1 to 5 according to  feasibility, novelty, clinical implications, and relevance to current practice. A  total of 26 experts (58%) completed the survey. The mean (± sd) rankings of  research ideas were 4.03 (± 0.17) for feasibility, 3.5 (± 0.17) for potential  impact on the field, 3.84 (± 0.12) for clinical relevance, and 3.08 (± 0.36) for  novelty and innovation. Results of this study suggest that ChatGPT offers a  promising approach to generating research ideas in dysphagia. While its current  capability to generate innovative ideas appears limited, it can serve as a  supplementary resource for researchers."
2023,Can ChatGPT Guide Parents on Tympanostomy Tube Insertion?,"Children (Basel, Switzerland)","BACKGROUND: The emergence of ChatGPT, a state-of-the-art language model developed by OpenAI, has introduced a novel avenue for patients to seek medically related  information. This technology holds significant promise in terms of accessibility  and convenience. However, the use of ChatGPT as a source of accurate information  enhancing patient education and engagement requires careful consideration. The  objective of this study was to assess the accuracy and reliability of ChatGPT in  providing information on the indications and management of complications  post-tympanostomy, the most common pediatric procedure in otolaryngology.  METHODS: We prompted ChatGPT-3.5 with questions and compared its generated  responses with the recommendations provided by the latest American Academy of  Otolaryngology-Head and Neck Surgery Foundation (AAO-HNSF) ""Clinical Practice  Guideline: Tympanostomy Tubes in Children (Update)"". RESULTS: A total of 23  responses were generated by ChatGPT against the AAO-HNSF guidelines. Following a  thorough review, it was determined that 22/23 (95.7%) responses exhibited a high  level of reliability and accuracy, closely aligning with the gold standard.  CONCLUSION: Our research study indicates that ChatGPT may be of assistance to  parents in search of information regarding tympanostomy tube insertion and its  clinical implications."
2024,Is ChatGPT-4 Accurate in Proofread a Manuscript in Otolaryngology-Head and Neck Surgery?,Otolaryngology--head and neck surgery : official journal of American Academy of Otolaryngology-Head and Neck Surgery,"ChatGPT is a new artificial intelligence-powered language model of chatbot able to help otolaryngologists in clinical practice and research. We investigated the  ability of ChatGPT-4 in the editing of a manuscript in otolaryngology. Four  papers were written by a nonnative English otolaryngologist and edited by a  professional editing service. ChatGPT-4 was used to detect and correct errors in  manuscripts. From the 171 errors in the manuscripts, ChatGPT-4 detected 86 errors  (50.3%) including vocabulary (N = 36), determiner (N = 27), preposition (N = 24),  capitalization (N = 20), and number (N = 11). ChatGPT-4 proposed appropriate  corrections for 72 (83.7%) errors, while some errors were poorly detected (eg,  capitalization [5%] and vocabulary [44.4%] errors. ChatGPT-4 claimed to change  something that was already there in 82 cases. ChatGPT demonstrated usefulness in  identifying some types of errors but not all. Nonnative English researchers  should be aware of the current limits of ChatGPT-4 in the proofreading of  manuscripts."
2023,Validity of the large language model ChatGPT (GPT4) as a patient information source in otolaryngology by a variety of doctors in a tertiary  otorhinolaryngology department.,Acta oto-laryngologica,"BACKGROUND: A high number of patients seek health information online, and large language models (LLMs) may produce a rising amount of it. AIM: This study  evaluates the performance regarding health information provided by ChatGPT, a LLM  developed by OpenAI, focusing on its utility as a source for  otolaryngology-related patient information. MATERIAL AND METHOD: A variety of  doctors from a tertiary otorhinolaryngology department used a Likert scale to  assess the chatbot's responses in terms of accuracy, relevance, and depth. The  responses were also evaluated by ChatGPT. RESULTS: The composite mean of the  three categories was 3.41, with the highest performance noted in the relevance  category (mean = 3.71) when evaluated by the respondents. The accuracy and depth  categories yielded mean scores of 3.51 and 3.00, respectively. All the categories  were rated as 5 when evaluated by ChatGPT. CONCLUSION AND SIGNIFICANCE: Despite  its potential in providing relevant and accurate medical information, the  chatbot's responses lacked depth and were found to potentially perpetuate biases  due to its training on publicly available text. In conclusion, while LLMs show  promise in healthcare, further refinement is necessary to enhance response depth  and mitigate potential biases."
2024,BPPV Information on Google Versus AI (ChatGPT).,Otolaryngology--head and neck surgery : official journal of American Academy of Otolaryngology-Head and Neck Surgery,"OBJECTIVE: To quantitatively compare online patient education materials found using traditional search engines (Google) versus conversational Artificial  Intelligence (AI) models (ChatGPT) for benign paroxysmal positional vertigo  (BPPV). STUDY DESIGN: The top 30 Google search results for ""benign paroxysmal  positional vertigo"" were compared to the OpenAI conversational AI language model,  ChatGPT, responses for 5 common patient questions posed about BPPV in February  2023. Metrics included readability, quality, understandability, and  actionability. SETTING: Online information. METHODS: Validated online information  metrics including Flesch-Kincaid Grade Level (FKGL), Flesch Reading Ease (FRE),  DISCERN instrument score, and Patient Education Materials Assessment Tool for  Printed Materials were analyzed and scored by reviewers. RESULTS: Mean  readability scores, FKGL and FRE, for the Google webpages were 10.7 ± 2.6 and  46.5 ± 14.3, respectively. ChatGPT responses had a higher FKGL score of  13.9 ± 2.5 (P < .001) and a lower FRE score of 34.9 ± 11.2 (P = .005), both  corresponding to lower readability. The Google webpages had a DISCERN part 2  score of 25.4 ± 7.5 compared to the individual ChatGPT responses with a score of  17.5 ± 3.9 (P = .001), and the combined ChatGPT responses with a score of  25.0 ± 0.9 (P = .928). The average scores of the reviewers for all ChatGPT  responses for accuracy were 4.19 ± 0.82 and 4.31 ± 0.67 for currency. CONCLUSION:  The results of this study suggest that the information on ChatGPT is more  difficult to read, of lower quality, and more difficult to comprehend compared to  information on Google searches."
2023,Diagnostic and Management Applications of ChatGPT in Structured Otolaryngology Clinical Scenarios.,OTO open,"OBJECTIVE: To evaluate the clinical applications and limitations of chat generative pretrained transformer (ChatGPT) in otolaryngology. STUDY DESIGN:  Cross-sectional survey. SETTING: Tertiary academic center. METHODS: ChatGPT 4.0  was queried for diagnoses and management plans for 20 physician-written clinical  vignettes in otolaryngology. Attending physicians were then asked to rate the  difficulty of the clinical vignettes and agreement with the differential  diagnoses and management plans of ChatGPT responses on a 5-point Likert scale.  Summary statistics were calculated. Univariate ordinal regression was then  performed between vignette difficulty and quality of the diagnoses and management  plans. RESULTS: Eleven attending physicians completed the survey (61% response  rate). Overall, vignettes were rated as very easy to neutral difficulty (range of  median score: 1.00-4.00; overall median 2.00). There was a high agreement with  the differential diagnosis provided by ChatGPT (range of median score: 3.00-5.00;  overall median: 5.00). There was also high agreement with treatment plans (range  of median score: 3.00-5.00; overall median: 5.00). There was no association  between vignette difficulty and agreement with differential diagnosis or  treatment. Lower diagnosis scores had greater odds of having lower treatment  scores. CONCLUSION: Generative artificial intelligence models like ChatGPT are  being rapidly adopted in medicine. Performance with curated, easy-to-moderate  difficulty otolaryngology scenarios indicate high agreement with physicians for  diagnosis and management. However, a decreased quality in diagnosis is associated  with decreased quality in management. Further research is necessary on ChatGPT's  ability to handle unstructured clinical information."
2023,Harnessing the power of electronic health records and open natural language data mining to capture meaningful patient experience during routine clinical care.,International journal of pediatric otorhinolaryngology,"INTRODUCTION: Electronic health records (EHR) are a rich data source for both quality improvement and clinical research. Natural language processing can be  harnessed to extract data from these previously difficult to access sources.  OBJECTIVE: The objective of this study was to create and apply a natural language  search query to extract EHR data to ask and answer quality improvement questions  at a pediatric aerodigestive center. METHODS: We developed a combined natural  language search query to extract clinically meaningful data along with  International Statistical Classification of Diseases (ICD10) and Current  Procedural Terminology (CPT) code data. This search query was applied to a single  pediatric aerodigestive center to answer key clinical questions asked by  families. Data were extracted from EHR data from first clinic visit, operative  note, microbiology lab report, and pathology report for all new patients from  2020 to 2021. Included as three queries were: 1) if I bring my child to a  pediatric aerodigestive center, how often will my child obtain a medical  diagnosis without needing an intervention? 2) if my child has a diagnostic  procedure, how often will a diagnosis be made? 3) if a diagnosis is made, can it  be addressed during that endoscopic intervention? RESULTS: For the 711 new  patients coming to the pediatric aerodigestive center from 2020 to 2021, only  26-32% required an interventional triple endoscopy (rigid/flexible bronchoscopy  with esophagoduodenoscopy). Of these triple endoscopies, 75.7% resulted in a  positive finding that enabled optimization of that child's care. Of the 221  patients who underwent diagnostic triple endoscopies, 40.7% underwent  intervention at the same time for laryngeal cleft (injection or suture, dependent  upon age). CONCLUSION: Here we created an effective model of open language search  query to extract meaningful metrics of patient experience from EHR data. This  model easily allows the EHR to be harnessed to create retrospective and  prospective databases that can be readily queried to answer clinical questions  important to patients. Such databases are widely applicable not just to pediatric  aerodigestive centers but to any clinical care setting using an EHR."
2024,The role of ChatGPT in enhancing ENT surgical training - a trainees' perspective.,The Journal of laryngology and otology,"OBJECTIVE: ChatGPT, developed by Open AI (November 2022) is a powerful artificial intelligence language model, designed to produce human-like text from  user-written prompts. Prompts must give context-specific information to produce  valuable responses. Otolaryngology is a specialist field that sees limited  exposure during undergraduate and postgraduate education. Additionally,  otolaryngology trainees have seen a reduction in learning opportunities since the  coronavirus disease 2019 pandemic. METHOD: This article aims to give guidance on  optimising the ChatGPT system in the context of education for otolaryngology by  reviewing barriers to otolaryngology education and suggesting ways that ChatGPT  can overcome them by providing examples using the authors' experience. RESULTS:  Overall, the authors saw that ChatGPT demonstrated some useful qualities,  particularly with regards to assistance with communication skills and  individualised patient responses. CONCLUSION: Although ChatGPT cannot replace  traditional mentorship and practical surgical experience, it can serve as an  invaluable supplementary resource to education in otolaryngology."
2023,"""Vertigo, likely peripheral"": the dizzying rise of ChatGPT.",European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the  German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,"PURPOSE: The use of artificial intelligence (AI) in medical decision-making has once again come to the forefront with the prevalence of Natural Language  Processing (NLP). In this exploratory article, we tested one such model, ChatGPT,  for its ability to identify vestibular causes of dizziness METHODS: Eight  hypothetical scenarios were presented to ChatGPT, which included varying clinical  pictures and types of prompts. The responses given by ChatGPT were evaluated for  coherence, clarity, consistency, accuracy, appropriateness, and recognition of  limitations. ChatGPT provided coherent and logical responses. RESULTS: The model  accurately provided differentials for both vestibular and non-vestibular causes  of dizziness, with the correct diagnosis presented first in six of the cases,  with important limitations CONCLUSION: Being an AI tool, ChatGPT lacks the  ability to process certain nuances in clinical decision making, in both  identifying atypical dizziness, as well as in recommending further examination  steps to elucidate a clearer diagnosis. We believe that AI will continue to forge  ahead in the medical field. Merging the immense knowledge base of AI programming  with the nuances of clinical assessment and knowledge integration will surely  enhance patient care in the years to come."
2023,Automated Identification of Aspirin-Exacerbated Respiratory Disease Using Natural Language Processing and Machine Learning: Algorithm Development and Evaluation  Study.,JMIR AI,"BACKGROUND: Aspirin-exacerbated respiratory disease (AERD) is an acquired inflammatory condition characterized by the presence of asthma, chronic  rhinosinusitis with nasal polyposis, and respiratory hypersensitivity reactions  on ingestion of aspirin or other nonsteroidal anti-inflammatory drugs (NSAIDs).  Despite AERD having a classic constellation of symptoms, the diagnosis is often  overlooked, with an average of greater than 10 years between the onset of  symptoms and diagnosis of AERD. Without a diagnosis, individuals will lack  opportunities to receive effective treatments, such as aspirin desensitization or  biologic medications. OBJECTIVE: Our aim was to develop a combined algorithm that  integrates both natural language processing (NLP) and machine learning (ML)  techniques to identify patients with AERD from an electronic health record (EHR).  METHODS: A rule-based decision tree algorithm incorporating NLP-based features  was developed using clinical documents from the EHR at Mayo Clinic. From clinical  notes, using NLP techniques, 7 features were extracted that included the  following: AERD, asthma, NSAID allergy, nasal polyps, chronic sinusitis, elevated  urine leukotriene E4 level, and documented no-NSAID allergy. MedTagger was used  to extract these 7 features from the unstructured clinical text given a set of  keywords and patterns based on the chart review of 2 allergy and immunology  experts for AERD. The status of each extracted feature was quantified by  assigning the frequency of its occurrence in clinical documents per subject. We  optimized the decision tree classifier's hyperparameters cutoff threshold on the  training set to determine the representative feature combination to discriminate  AERD. We then evaluated the resulting model on the test set. RESULTS: The AERD  algorithm, which combines NLP and ML techniques, achieved an area under the  receiver operating characteristic curve score, sensitivity, and specificity of  0.86 (95% CI 0.78-0.94), 80.00 (95% CI 70.82-87.33), and 88.00 (95% CI  79.98-93.64) for the test set, respectively. CONCLUSIONS: We developed a  promising AERD algorithm that needs further refinement to improve AERD diagnosis.  Continued development of NLP and ML technologies has the potential to reduce  diagnostic delays for AERD and improve the health of our patients."
2023,GPT-4 accuracy and completeness against International Consensus Statement on Allergy and Rhinology: Rhinosinusitis.,International forum of allergy & rhinology,GPT-4 is an AI language model that can answer basic questions about rhinologic disease. Vetting is needed before AI models can be safely integrated into  otolarygologic patient care.
2023,Bilateral Vocal Fold Paralysis in a Patient With Neurosarcoidosis: A ChatGPT-Driven Case Report Describing an Unusual Presentation.,Cureus,"This ChatGPT-driven case report describes a unique presentation of neurosarcoidosis. The patient, a 58-year-old female, initially presented with  hoarseness and was found to have bilateral jugular foramen tumors and thoracic  lymphadenopathy. Imaging revealed significant enlargement and thickening of the  vagus nerve and a separate mass of the cervical sympathetic trunk. The patient  was referred for an ultrasound-guided biopsy of the abnormal neck masses to  establish a pathologic diagnosis. The patient subsequently underwent neck  dissection for exposure of the vagus nerve and isolation of the great vessels in  preparation for a transmastoid approach to the skull base. The presence of  multifocal tumors prompted the need for a biopsy, which ultimately revealed  sarcoid granulomas in the nervous system. The patient was diagnosed with  neurosarcoidosis. This case highlights the potential for sarcoidosis to affect  the nervous system, with multiple cranial nerve involvement, seizures, and  cognitive impairment. It also emphasizes the need for a combination of clinical,  radiological, and pathological findings for an accurate diagnosis of  neurosarcoidosis. Additionally, this case highlights the utility of natural  language processing (NLP), as the entire case report was written using ChatGPT.  This report serves as a comparison of the quality of case reports generated by  humans versus NLP algorithms. The original case report can be found in the  references."
2023,A natural language processing approach to uncover patterns among online ratings of otolaryngologists.,The Journal of laryngology and otology,"BACKGROUND: Patients increasingly use physician rating websites to evaluate and choose potential healthcare providers. A sentiment analysis and machine learning  approach can uniquely analyse written prose to quantitatively describe patients'  perspectives from interactions with their physicians. METHODS: Online written  reviews and star scores were analysed from Healthgrades.com using a natural  language processing sentiment analysis package. Demographics of otolaryngologists  were compared and a multivariable regression for individual words was performed.  RESULTS: This study analysed 18 546 online reviews of 1240 otolaryngologists  across the USA. Younger otolaryngologists (aged less than 40 years) had higher  sentiment and star scores compared with older otolaryngologists (p < 0.001). Male  otolaryngologists had higher sentiment and star scores compared with female  otolaryngologists (p < 0.001). 'Confident', 'kind', 'recommend' and 'comfortable'  were words associated with positive reviews (p < 0.001). CONCLUSION: Positive  bedside manner was strongly reflected in better reviews, and younger age and male  gender of the otolaryngologist were associated with better sentiment and star  scores."
2023,Exploring patient experiences and concerns in the online Cochlear implant community: A cross-sectional study and validation of automated topic modelling.,Clinical otolaryngology : official journal of ENT-UK ; official journal of Netherlands Society for Oto-Rhino-Laryngology & Cervico-Facial Surgery,"OBJECTIVE: There is a paucity of research examining patient experiences of cochlear implants. We sought to use natural language processing methods to  explore patient experiences and concerns in the online cochlear implant (CI)  community. MATERIALS AND METHODS: Cross-sectional study of posts on the online  Reddit r/CochlearImplants forum from 1 March 2015 to 11 November 2021. Natural  language processing using the BERTopic automated topic modelling technique was  employed to cluster posts into semantically similar topics. Topic categorisation  was manually validated by two independent reviewers and Cohen's kappa calculated  to determine inter-rater reliability between machine vs human and human vs human  categorisation. RESULTS: We retrieved 987 posts from 588 unique Reddit users on  the r/CochlearImplants forum. Posts were initially categorised by BERTopic into  16 different Topics, which were increased to 23 Topics following manual  inspection. The most popular topics related to CI connectivity (n = 112), adults  considering getting a CI (n = 107), surgery-related posts (n = 89) and day-to-day  living with a CI (n = 85). Cohen's kappa among all posts was 0.62 (machine vs.  human) and 0.72 (human vs. human), and among categorised posts was 0.85 (machine  vs. human) and 0.84 (human vs. human). CONCLUSIONS: This cross-sectional study of  social media discussions among the online cochlear implant community identified  common attitudes, experiences and concerns of patients living with, or seeking, a  cochlear implant. Our validation of natural language processing methods to  categorise topics shows that automated analysis of similar Otolaryngology-related  content is a viable and accurate alternative to manual qualitative approaches."
2023,Keyword-augmented and semi-automatic generation of FESS reports: a proof-of-concept study.,International journal of computer assisted radiology and surgery,"INTRODUCTION: Surgical reports are usually written after a procedure and must often be reproduced from memory. Thus, this is an error-prone, and time-consuming  task which increases the workload of physicians. In this proof-of-concept study,  we developed and evaluated a software tool using Artificial Intelligence (AI) for  semi-automatic intraoperative generation of surgical reports for functional  endoscopic sinus surgery (FESS). MATERIALS AND METHODS: A vocabulary of keywords  for developing a neural language model was created. With an  encoder-decoder-architecture, artificially coherent sentence structures, as they  would be expected in general operation reports, were generated. A first set of 48  conventional operation reports were used for model training. After training, the  reports were generated again and compared to those before training. Established  metrics were used to measure optimization of the model objectively. A cohort of  16 physicians corrected and evaluated three randomly selected, generated reports  in four categories: ""quality of the generated operation reports,"" ""time-saving,""  ""clinical benefits"" and ""comparison with the conventional reports."" The  corrections of the generated reports were counted and categorized. RESULTS:  Objective parameters showed improvement in performance after training the  language model (p < 0.001). 27.78% estimated a timesaving of 1-15 and 61.11% of  16-30 min per day. 66.66% claimed to see a clinical benefit and 61.11% a relevant  workload reduction. Similarity in content between generated and conventional  reports was seen by 33.33%, similarity in form by 27.78%. 66.67% would use this  tool in the future. An average of 23.25 ± 12.5 corrections was needed for a  subjectively appropriate surgery report. CONCLUSION: The results indicate  existing limitations of applying deep learning to text generation of operation  reports and show a high acceptance by the physicians. By taking over this  time-consuming task, the tool could reduce workload, optimize clinical workflows  and improve the quality of patient care. Further training of the language model  is needed."
2022,A Machine Learning Model to Predict Citation Counts of Scientific Papers in Otology Field.,BioMed research international,"One of the most widely used measures of scientific impact is the number of citations. However, due to its heavy-tailed distribution, citations are  fundamentally difficult to predict but can be improved. This study was aimed at  investigating the factors and parts influencing the citation number of a  scientific paper in the otology field. Therefore, this work proposes a new  solution that utilizes machine learning and natural language processing to  process English text and provides a paper citation as the predicted results.  Different algorithms are implemented in this solution, such as linear regression,  boosted decision tree, decision forest, and neural networks. The application of  neural network regression revealed that papers' abstracts have more influence on  the citation numbers of otological articles. This new solution has been developed  in visual programming using Microsoft Azure machine learning at the back end and  Programming Without Coding Technology at the front end. We recommend using  machine learning models to improve the abstracts of research articles to get more  citations."
2023,A Deep Learning Model for Classification of Parotid Neoplasms Based on Multimodal Magnetic Resonance Image Sequences.,The Laryngoscope,"OBJECTIVE: To design a deep learning model based on multimodal magnetic resonance image (MRI) sequences for automatic parotid neoplasm classification, and to  improve the diagnostic decision-making in clinical settings. METHODS: First,  multimodal MRI sequences were collected from 266 patients with parotid neoplasms,  and an artificial intelligence (AI)-based deep learning model was designed from  scratch, combining the image classification network of Resnet and the Transformer  network of Natural language processing. Second, the effectiveness of the deep  learning model was improved through the multi-modality fusion of MRI sequences,  and the fusion strategy of various MRI sequences was optimized. In addition, we  compared the effectiveness of the model in the parotid neoplasm classification  with experienced radiologists. RESULTS: The deep learning model delivered  reliable outcomes in differentiating benign and malignant parotid neoplasms. The  model, which was trained by the fusion of T2-weighted, postcontrast T1-weighted,  and diffusion-weighted imaging (b = 1000 s/mm(2) ), produced the best result,  with an accuracy score of 0.85, an area under the receiver operator  characteristic (ROC) curve of 0.96, a sensitivity score of 0.90, and a  specificity score of 0.84. In addition, the multi-modal paradigm exhibited  reliable outcomes in diagnosing the pleomorphic adenoma and the Warthin tumor,  but not in the identification of the basal cell adenoma. CONCLUSION: An accurate  and efficient AI based classification model was produced to classify parotid  neoplasms, resulting from the fusion of multimodal MRI sequences. The  effectiveness certainly outperformed the model with single MRI images or single  MRI sequences as input, and potentially, experienced radiologists. LEVEL OF  EVIDENCE: 3 Laryngoscope, 133:327-335, 2023."
2023,Advances in Artificial Intelligence to Diagnose Otitis Media: State of the Art Review.,Otolaryngology--head and neck surgery : official journal of American Academy of Otolaryngology-Head and Neck Surgery,"OBJECTIVE: Otitis media (OM) is a model disease for developing, validating, and implementing artificial intelligence (AI) techniques. We aim to review the state  of the art applications of AI used to diagnose OM in pediatric and adult  populations. DATA SOURCES: Several comprehensive databases were searched to  identify all articles that applied AI technologies to diagnose OM. REVIEW  METHODS: Relevant articles from January 2010 through May 2021 were identified by  title and abstract. Articles were excluded if they did not discuss AI in  conjunction with diagnosing OM. References of included studies and relevant  review articles were cross-referenced to identify any additional studies.  CONCLUSION: Title and abstract screening resulted in full-text retrieval of 40  articles that met initial screening parameters. Of this total, secondary review  articles (n = 7) and commentary-based articles (n = 2) were removed, as were  articles that did not specifically discuss AI and OM diagnosis (n = 5), leaving  25 articles for review. Applications of AI technologies specific to diagnosing OM  included machine learning and natural language processing (n = 23) and prototype  approaches (n = 2). IMPLICATIONS FOR PRACTICE: This review emphasizes the utility  of AI techniques to automate and aid in diagnosing OM. Although these techniques  are still in the development and testing stages, AI has the potential to improve  the practice of otolaryngologists and primary care clinicians by increasing the  efficiency and accuracy of diagnoses."
2022,Online Discussions About Tinnitus: What Can We Learn From Natural Language Processing of Reddit Posts?,American journal of audiology,"BACKGROUND: This study was aimed at identifying key topics in online discussions about tinnitus by examining a large data set extracted from Reddit social media  using a natural language processing technique. METHOD: A corpus of 113,215 posts  about tinnitus was extracted from Reddit's application programming interface.  After cleaning the data for duplications and posts without any text information,  the sample was reduced to 101,905 posts, which was subjected to cluster analysis  using the open-source IRaMuTeQ software to identify main topics based on the  co-occurrence of texts. These clusters were named by a panel of tinnitus experts  (n = 9) by reading typical text segments within each cluster. RESULTS: The  cluster analysis identified 16 unique clusters that belong to two topics, which  were named ""tinnitus causes and consequences"" and ""tinnitus management and  coping."" Based on their characteristics, the clusters were named: tinnitus  timeline (10%), tinnitus perception (9.7%), medical triggers and modulators  (8.8%), hearing research (8.8%), attention and silence (8.6%), social media posts  about tinnitus (7.4%), hearing protection (7.3%), interaction with hearing health  care providers (6.7%), mental health and coping (5.8%), music listening (5.7%),  hope for a cure (5.6%), interactions with people without tinnitus (5.4%), dietary  supplements and alternative therapies (3.2%), sleep (3.9%), dietary effects  (1.7%), and writing about tinnitus and being thankful to online community (1.4%).  CONCLUSIONS: Despite some limitations, tinnitus posts on Reddit provide rich  real-world data to identify various issues and complaints that tinnitus patients  and their significant others discuss in online communities. Some of the clusters  identified here are novel (e.g., tinnitus timeline, interactions with people  without tinnitus) and have not been much discussed in the tinnitus literature.  The results suggest that individuals with tinnitus relay on social media for  support and highlight the service delivery needs in providing social support  through other means (e.g., support groups)."
2021,Data Mining of Free-Text Responses: An Innovative Approach to Analyzing Patient Perspectives on Treatment for Chronic Rhinosinusitis with Nasal Polyps in a Phase  IIa Proof-of-Concept Study for Dupilumab.,Patient preference and adherence,"PURPOSE: Patient perspective is an important and increasingly sought-after complement to clinical assessment. The aim of this study was to transcribe  individual patients' experience of treatment in a dupilumab clinical trial  through free-text responses with analysis using natural language processing (NLP)  to obtain the unique perspective of patients on disease impact and unmet needs  with existing treatment to inform future trial design. PATIENTS AND METHODS:  Patients with chronic rhinosinusitis with nasal polyps (CRSwNP) who were enrolled  in a Phase IIa randomized controlled trial comparing dupilumab with placebo  (NCT01920893) were invited to complete a self-assessment of treatment (SAT) tool  at the end of treatment, asking, ""What is your opinion on the treatment you had  during the trial? What did you like or dislike about the treatment?"" Free-text  responses were analyzed for the overall cohort and according to treatment  assignment using natural language processing including sentiment scoring. In a  mixed-methods approach, quantitative patient-reported outcome (PRO) results were  utilized to complement the qualitative analysis of free-text responses. RESULTS:  Of 60 patients enrolled in the study, 43 (71.6%) completed the SAT and responses  from 37 patients were analyzed (placebo, n = 16; dupilumab, n = 21). Word  analyses showed that the most common words were ""smell,"" ""improve,"" ""staff,""  ""great,"" ""time,"" and ""good."" Across the whole cohort, ""smell"" was the most common  symptom-related word. The words ""smell"" and ""experience"" were more likely to  occur in patients treated with dupilumab. Patients treated with dupilumab also  had more positive sentiment in their SAT responses than those who received  placebo. The results from this qualitative analysis were reflected in  quantitative PRO results. CONCLUSION: ""Smell"" was important to patients with  CRSwNP, highlighting its importance as a patient-centric efficacy outcome measure  in the context of clinical trials in CRSwNP. TRIAL REGISTRATION:  ClinicalTrials.gov, NCT01920893. Registered 12 August 2013,  https://www.clinicaltrials.gov/ct2/show/NCT01920893."
2020,Language-based translation and prediction of surgical navigation steps for endoscopic wayfinding assistance in minimally invasive surgery.,International journal of computer assisted radiology and surgery,"PURPOSE: In the context of aviation and automotive navigation technology, assistance functions are associated with predictive planning and wayfinding  tasks. In endoscopic minimally invasive surgery, however, assistance so far  relies primarily on image-based localization and classification. We show that  navigation workflows can be described and used for the prediction of navigation  steps. METHODS: A natural description vocabulary for observable anatomical  landmarks in endoscopic images was defined to create 3850 navigation workflow  sentences from 22 annotated functional endoscopic sinus surgery (FESS)  recordings. Resulting FESS navigation workflows showed an imbalanced data  distribution with over-represented landmarks in the ethmoidal sinus. A  transformer model was trained to predict navigation sentences in  sequence-to-sequence tasks. The training was performed with the Adam optimizer  and label smoothing in a leave-one-out cross-validation study. The sentences were  generated using an adapted beam search algorithm with exponential decay beam  rescoring. The transformer model was compared to a standard  encoder-decoder-model, as well as HMM and LSTM baseline models. RESULTS: The  transformer model reached the highest prediction accuracy for navigation steps at  0.53, followed by 0.35 of the LSTM and 0.32 for the standard  encoder-decoder-network. With an accuracy of sentence generation of 0.83, the  prediction of navigation steps at sentence-level benefits from the additional  semantic information. While standard class representation predictions suffer from  an imbalanced data distribution, the attention mechanism also considered  underrepresented classes reasonably well. CONCLUSION: We implemented a natural  language-based prediction method for sentence-level navigation steps in  endoscopic surgery. The sentence-level prediction method showed a potential that  word relations to navigation tasks can be learned and used for predicting future  steps. Further studies are needed to investigate the functionality of path  prediction. The prediction approach is a first step in the field of  visuo-linguistic navigation assistance for endoscopic minimally invasive surgery."
2018,Unique Clinical Language Patterns Among Expert Vestibular Providers Can Predict Vestibular Diagnoses.,"Otology & neurotology : official publication of the American Otological Society, American Neurotology Society [and] European Academy of Otology and Neurotology","OBJECTIVE: To identify novel language usage by expert providers predictive of specific vestibular conditions. STUDY DESIGN: Retrospective chart review and  natural language processing. Level IV. SETTING: Tertiary referral center.  PATIENTS: Patients seen for vestibular complaint. INTERVENTION(S): Natural  language processing and machine learning analyses of semantic and syntactic  patterns in clinical documentation from vestibular patients. MAIN OUTCOME  MEASURE: Accuracy of Naïve Bayes predictive models correlating language usage  with clinical diagnoses. RESULTS: Natural language analyses on 866  physician-generated histories from vestibular patients found 3,286 unique  examples of language usage of which 614 were used 10 or greater times. The top 15  semantic types represented only 11% of all Unified Medical Language System  semantic types but covered 86% of language used in vestibular patient histories.  Naïve Bayes machine learning algorithms on a subset of 255 notes representing  benign paroxysmal positional vertigo, vestibular migraine, anxiety-related  dizziness and central dizziness generated strong predictive models showing an  average sensitivity rate of 93.4% and a specificity rate of 98.2%. A binary model  for assessing whether a subject had a specific diagnosis or not had an average  AUC for the receiver operating characteristic curves of .995 across all  conditions. CONCLUSIONS: These results indicate that expert providers utilize  unique language patterns in vestibular notes that are highly conserved. These  patterns have strong predictive power toward specific vestibular diagnoses. Such  language elements can provide a simple vocabulary to aid nonexpert providers in  formulating a differential diagnosis. They can also be incorporated into clinical  decision support systems to facilitate accurate vestibular diagnosis in  ambulatory settings."
2016,Toward Signaling-Driven Biomarkers Immune to Normal Tissue Contamination.,Cancer informatics,"The goal of this study was to discover a minimally invasive pathway-specific biomarker that is immune to normal cell mRNA contamination for diagnosing head  and neck squamous cell carcinoma (HNSCC). Using Elsevier's MedScan natural  language processing component of the Pathway Studio software and the TRANSFAC  database, we produced a curated set of genes regulated by the signaling networks  driving the development of HNSCC. The network and its gene targets provided prior  probabilities for gene expression, which guided our CoGAPS matrix factorization  algorithm to isolate patterns related to HNSCC signaling activity from a  microarray-based study. Using patterns that distinguished normal from tumor  samples, we identified a reduced set of genes to analyze with Top Scoring Pair in  order to produce a potential biomarker for HNSCC. Our proposed biomarker  comprises targets of the transcription factor (TF) HIF1A and the FOXO family of  TFs coupled with genes that show remarkable stability across all normal tissues.  Based on validation with novel data from The Cancer Genome Atlas (TCGA), measured  by RNAseq, and bootstrap sampling, the biomarker for normal vs. tumor has an  accuracy of 0.77, a Matthews correlation coefficient of 0.54, and an area under  the curve (AUC) of 0.82."
