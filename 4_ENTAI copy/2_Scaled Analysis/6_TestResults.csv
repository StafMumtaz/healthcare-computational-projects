Publication Year,Title,Publication Title,Abstract Note,nlp_ent_relevant,real_world_app,substantive_study,research_tool,data_insights,clinical_decision,emr_integration
2021,How To Use Zotero (A Complete Beginner's Guide),,,0,0,0,0,0,0,0
2025,Innovations in Otolaryngology Using LLM for Early Detection of Sleep-Disordered Breathing.,SLAS technology,"Sleep Disordered Breathing (SDB), including conditions like Obstructive Sleep Apnea (OSA), represents a major health concern, characterized by irregular  airflow during sleep due to airway obstruction. SDB can result in serious health  problems. Implementation of early intervention is vital whenever patient outcomes  are to be considered. This research aims to advance research on otolaryngology  using Machine Learning (ML) models, and Large Language Models (LLM) for  identification of SDB using Electronic Health Record (HER). The approach proposes  a hybrid ML framework combining the Dynamic Seagull Search algorithm-driven Large  Language model (DSS-LLM). The extensive clinical dataset is used to train the  model. It includes patient demographics, medical history, sleep habits,  comorbidities, and physical measurements. Data pre-processing involves handling  missing values, applying NLP techniques, and normalization. Feature extraction is  done using Principal Component Analysis (PCA) to reduce the dimensionality of the  hyperparameters and finally for selecting the best set of predictors. The  extracted features are then used to train the proposed DSS-LLM model, which  incorporates the DSS algorithm to optimize the LLM classifier, improving  classification accuracy and model robustness. Subsequently, the idea of LLM is  introduced for its application on textual clinical records comprising physicians'  reports and patients' symptoms. The findings from an experiment suggest that the  proposed model enhances the classification accuracy achieved to 98.91 %,  precision attained by 98.9 %, recall achieved to 98.92 % and F-1 score attained  by 98.58 % as compared to the models developed earlier. This research provides a  novel solution to the screening of OSA at the pre-clinical level which involves  hybrid machine learning models integrated with LLMs. This proposed framework is  expected to boost clinical judgment and thereby increase better ophthalmology  outcomes for patients.",0,1,1,0,1,1,0
2025,"Comparison of ChatGPT-4, Copilot, Bard and Gemini Ultra on an Otolaryngology Question Bank.",Clinical otolaryngology : official journal of ENT-UK ; official journal of Netherlands Society for Oto-Rhino-Laryngology & Cervico-Facial Surgery,"OBJECTIVE: To compare the performance of Google Bard, Microsoft Copilot, GPT-4 with vision (GPT-4) and Gemini Ultra on the OTO Chautauqua, a student-created,  faculty-reviewed otolaryngology question bank. STUDY DESIGN: Comparative  performance evaluation of different LLMs. SETTING: N/A. PARTICIPANTS: N/A.  METHODS: Large language models (LLMs) are being extensively tested in medical  education. However, their accuracy and effectiveness remain understudied,  particularly in otolaryngology. This study involved inputting 350  single-best-answer multiple choice questions, including 18 image-based questions,  into four LLMS. Questions were sourced from six independent question banks  related to (a) rhinology, (b) head and neck oncology, (c) endocrinology, (d)  general otolaryngology, (e) paediatrics, (f) otology, (g) facial plastics,  reconstruction and (h) trauma. LLMs were instructed to provide an output  reasoning for their answers, the length of which was recorded. RESULTS: Aggregate  and subgroup analysis revealed that Gemini (79.8%) outperformed the other LLMs,  followed by GPT-4 (71.1%), Copilot (68.0%), and Bard (65.1%) in accuracy. The  LLMs had significantly different average response lengths, with Bard  (x̄ = 1685.24) being the longest and no difference between GPT-4 (x̄ = 827.34)  and Copilot (x̄ = 904.12). Gemini's longer responses (x̄ =1291.68) included  explanatory images and links. Gemini and GPT-4 correctly answered image-based  questions (n = 18), unlike Copilot and Bard, highlighting their adaptability and  multimodal capabilities. CONCLUSION: Gemini outperformed the other LLMs in terms  of accuracy, followed by GPT-4, Copilot and Bard. GPT-4, although it has the  second-highest accuracy, provides concise and relevant explanations. Despite the  promising performance of LLMs, medical learners should cautiously assess accuracy  and decision-making reliability.",1,0,1,0,0,0,0
2025,"Leveraging FDA Labeling Documents and Large Language Model to Enhance Annotation, Profiling, and Classification of Drug Adverse Events with AskFDALabel.",Drug safety,"BACKGROUND: Drug adverse events (AEs) represent a significant public health concern. US Food and Drug Administration (FDA) drug labeling documents are an  essential resource for studying drug safety such as assessing a drug's likelihood  to cause certain organ toxicities; however, the manual extraction of AEs is  labor-intensive, requires specialized expertise, and is challenging to maintain,  due to frequent updates of the labeling documents. OBJECTIVE: To automate the  extraction of AE data from FDA drug labeling documents, we developed a workflow  based on AskFDALabel, a large language model (LLM)-powered framework, and its  demonstration in drug safety studies. METHODS: This framework incorporates a  retrieval-augmented generation (RAG) component based on FDALabel to enhance  standard LLM inference. Key steps include (1) selection of a task-specific  template, (2) FDALabel database querying, and (3) content preparation for LLM  processing. We evaluated the performance of the framework in three benchmark  experiments, including drug-induced liver injury (DILI) classification,  drug-induced cardiotoxicity (DICT) classification, and AE term recognition.  RESULTS: AskFDALabel achieved F1-scores of 0.978 for DILI, 0.931 for DICT, and  0.911 for AE annotation, outperforming other traditional methods. It also  provided cited labeling content and detailed explanations, facilitating manual  verification. CONCLUSION: AskFDALabel exhibited high consistency with human AE  annotation, particularly in classifying and profiling DILI and DICT. Thus, it can  significantly enhance the efficiency and accuracy of AE annotation, with  promising potential for advanced AE surveillance and drug safety research.",0,1,1,1,1,0,0
2024,Voice EHR: introducing multimodal audio data for health.,Frontiers in digital health,"INTRODUCTION: Artificial intelligence (AI) models trained on audio data may have the potential to rapidly perform clinical tasks, enhancing medical  decision-making and potentially improving outcomes through early detection.  Existing technologies depend on limited datasets collected with expensive  recording equipment in high-income countries, which challenges deployment in  resource-constrained, high-volume settings where audio data may have a profound  impact on health equity. METHODS: This report introduces a novel protocol for  audio data collection and a corresponding application that captures health  information through guided questions. RESULTS: To demonstrate the potential of  Voice EHR as a biomarker of health, initial experiments on data quality and  multiple case studies are presented in this report. Large language models (LLMs)  were used to compare transcribed Voice EHR data with data (from the same  patients) collected through conventional techniques like multiple choice  questions. Information contained in the Voice EHR samples was consistently rated  as equally or more relevant to a health evaluation. DISCUSSION: The HEAR  application facilitates the collection of an audio electronic health record  (""Voice EHR"") that may contain complex biomarkers of health from conventional  voice/respiratory features, speech patterns, and spoken language with semantic  meaning and longitudinal context-potentially compensating for the typical  limitations of unimodal clinical datasets.",0,1,1,0,1,0,1
